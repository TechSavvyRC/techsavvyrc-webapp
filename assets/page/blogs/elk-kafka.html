<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta content="width=device-width, initial-scale=1.0" name="viewport">
        <title>TechSavvy</title>
        <meta content="" name="description">
        <meta content="" name="keywords">
        <!-- Favicons -->
        <link href="../../img/my_logo.png" rel="icon">
        <link href="../../img/apple-touch-icon.png" rel="apple-touch-icon">
        <!-- Fonts -->
        <link href="https://fonts.googleapis.com" rel="preconnect">
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Raleway:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">
        <!-- Custom Icons -->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/simple-icons@v7/icons.min.css">
        <!-- Vendor CSS Files -->
        <link href="../../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
        <link href="../../vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
        <link href="../../vendor/aos/aos.css" rel="stylesheet">
        <link href="../../vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
        <link href="../../vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
        <!-- Main CSS File -->
        <link href="../../css/blog_page.css" rel="stylesheet">
    </head>
    <body class="blog-page">
        <header class="sidebar dark-background">
            <div class="logo d-flex justify-content-center">
                <a href="../../../index.html" class="home-icon-link" aria-label="Home">
                <i class="bi bi-house navicon" style="color:#6c757d; font-size: 2.0rem; line-height:1;" ></i>
                </a>
            </div>
            <nav id="navmenu" class="navmenu">
                <h2 class="sitename" style="text-align: center; margin: 20px auto;">CONTENTS</h2>
                <ul>
                    <li> <a href="#overview" class="active">Overview</a> </li>
                    <li> <a href="#setup-guide">Setup Guide</a> </li>
                    <li> <a href="#docker-commands">Docker Commands</a> </li>
                    <li> <a href="#containers">Containers</a> </li>
                    <li> <a href="#network-config">Network Config</a> </li>
                    <li> <a href="#host-machine">Host Machine</a> </li>
                    <li> <a href="#architecture">Architecture</a> </li>
                    <li> <a href="#data-flow">Data Flow</a> </li>
                    <li> <a href="#directory-structure">Directory Structure</a> </li>
                    <li><a href="../../doc/Kafka-ELK_Docker.pdf" download="Kafka-ELK_Docker.pdf"><i class="bi bi-download navicon"></i> Download Document</a></li>
                </ul>
            </nav>
        </header>
        <main class="main-content">
            <!-- Blog Section -->
            <section id="blog" class="blog blog-section" style="justify-content: left;  padding: 0px 0;">
                <div>
                    <h1 style="font-size: 50px; border-bottom: none; text-decoration: none;" class="h1 dark-background">Kafka-ELK Stack with Docker: A Comprehensive Guide</h1>
                </div>
            </section>
            <!-- 1. Overview -->
            <section id="overview" class="blog-section">
                <h2 class="collapsible main-heading">1. Overview</h2>
                <div class="content">
                    <p style="text-align: justify; margin-bottom: 20px;">This documentation outlines the process for setting up a multi-node ELK stack (Elasticsearch, Logstash, Kibana) using Docker, with additional containers for Zookeeper, Kafka, and a Python-based banking application. The banking app generates synthetic bank transaction data, which is sent to a Kafka topic and then ingested into Elasticsearch via Logstash. The ingested data is then visualized using Kibana dashboards.</p>
                    <p style="text-align: justify; margin-bottom: 20px;">This setup is intended to simulate a real-time streaming pipeline where a producer (banking app) pushes data to Kafka, and Logstash acts as a consumer that processes and sends the data to Elasticsearch. Kibana is used to analyze and visualize the data through customized dashboards.</p>
                </div>
            </section>
            <!-- 2. Setup Guide -->
            <section id="setup-guide" class="blog-section">
                <h2 class="collapsible main-heading">2. Setup Guide for Docker Containers</h2>
                <div class="content">
                    <!-- Subheading: Prerequisites -->
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">2.1 Prerequisites</h3>
                    <div class="content">
                        <p>Before starting the setup, ensure the following are installed on your system:</p>
                        <ul class="custom-bullets">
                            <li><i class="bi"></i><strong>Docker:</strong> Install Docker from the <a href="https://www.docker.com/get-started" target="_blank">official Docker website</a>.</li>
                            <li><i class="bi"></i><strong>Docker Compose:</strong> Install Docker Compose from the <a href="https://docs.docker.com/compose/install/" target="_blank">official Docker Compose installation guide</a>.</li>
                            <li><i class="bi"></i><strong>Git:</strong> If using <code>git clone</code> to retrieve repository files, ensure Git is installed. Download from <a href="https://git-scm.com/downloads" target="_blank">Git SCM</a>.</li>
                            <li><i class="bi"></i>Minimum 8GB RAM</li>
                            <li><i class="bi"></i>20GB free disk space</li>
                        </ul>
                        <p>For users unfamiliar with Docker, basic knowledge of Linux commands and Docker is recommended.</p>
                    </div>
                    <!-- Subheading: Prerequisites -->
                    <h3 class="collapsible subheading">2.2 Repository Structure</h3>
                    <div class="content">
                        <p>The repository <strong>TechSavvyRC/elk_docker</strong> contains the necessary files to configure and run the ELK stack, Kafka, and the Python banking app in Docker containers. Below is the structure of the repository:</p>
                        <p><a href="https://github.com/TechSavvyRC/elk_docker.git" target="_blank"><strong>Git Repository Link: </strong>https://github.com/TechSavvyRC/elk_docker.git</a></p>
                        <div class="code-block">
                            <pre>
<code class="language-bash">TechSavvyRC/elk_docker
│
├── docker-compose.yml            # Main orchestration file
├── .env                          # Global environment variables
│
├── banking/                      # Python banking app directory
│    ├── .env                     # Banking app environment file
│    ├── banking_app.template.py  # Python script template for generating bank app
│    ├── Dockerfile               # Dockerfile for the banking app container
│    ├── entrypoint.sh            # Script for configuring & running the banking app
│    └── requirements.txt         # Python dependencies for the banking app
│
└── logstash/                     # Logstash configuration directory
     ├── .env                     # Logstash environment file
     ├── Dockerfile               # Dockerfile for the Logstash container
     ├── entrypoint.sh            # Script for configuring and running Logstash
     └── logstash.template.conf   # Logstash configuration template
</code></pre>
                        </div>
                    </div>
                    <!-- Subheading: Retrieve Repository Files -->
                    <h3 class="collapsible subheading">2.3 Retrieve Repository Files</h3>
                    <div class="content">
                        <!-- Manual Download Section -->
                        <h4 style="margin-top: 0.5em;"><i class="bi bi-1-circle-fill"></i> Manual Download</h4>
                        <div class="indent">
                            <p>To manually download the repository files, follow these steps:</p>
                            <ol>
                                <li>Navigate to the repository’s GitHub page:
                                    <a href="https://github.com/TechSavvyRC/elk_docker" target="_blank">https://github.com/TechSavvyRC/elk_docker</a>.
                                </li>
                                <li>Click the <strong>Code</strong> button.</li>
                                <li>Select <strong>Download ZIP</strong> and save the file.</li>
                                <li>Extract the ZIP file on your local machine using your preferred tool (e.g., <code>unzip</code> or a graphical interface).</li>
                            </ol>
                        </div>
                        <!-- Using Git Command-Line Section -->
                        <h4 style="margin-top: 2em;"><i class="bi bi-2-circle-fill"></i> Using Git Command-Line</h4>
                        <p>To clone the repository using Git, follow these steps:</p>
                        <ol>
                            <li>Open a terminal.</li>
                            <li>Run the following command to clone the repository:</li>
                            <div class="code-block">
                                <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                                <pre><code class="language-bash">git clone https://github.com/TechSavvyRC/elk_docker.git</code></pre>
                            </div>
                            <li>Navigate into the cloned directory:</li>
                            <div class="code-block">
                                <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                                <pre><code class="language-bash">cd elk_docker</code></pre>
                            </div>
                        </ol>
                        <!-- Using wget Section -->
                        <h4 style="margin-top: 2em;"><i class="bi bi-3-circle-fill"></i> Using wget</h4>
                        <p>If you prefer using <code>wget</code> to download the repository, follow these steps:</p>
                        <ol>
                            <li>Open a terminal.</li>
                            <li>Run the following command to download the repository’s ZIP file:</li>
                            <div class="code-block">
                                <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                                <pre><code class="language-bash">wget https://github.com/TechSavvyRC/elk_docker/archive/refs/heads/main.zip</code></pre>
                            </div>
                            <li>Extract the ZIP file and navigate into the directory:</li>
                            <div class="code-block">
                                <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                                <pre><code class="language-bash">unzip main.zip
cd elk_docker-main</code></pre>
                            </div>
                        </ol>
                        <!-- Using GitHub Desktop Section -->
                        <h4 style="margin-top: 2em;"><i class="bi bi-4-circle-fill"></i> Using GitHub Desktop</h4>
                        <p>If you are using GitHub Desktop, follow these steps:</p>
                        <ol>
                            <li>Download and install <a href="https://desktop.github.com/" target="_blank">GitHub Desktop</a>.</li>
                            <li>Click <strong>File &gt; Clone Repository</strong>.</li>
                            <li>Paste the repository URL:
                                <a href="https://github.com/TechSavvyRC/elk_docker.git" target="_blank">https://github.com/TechSavvyRC/elk_docker.git</a>.
                            </li>
                            <li>Choose a local path and click <strong>Clone</strong>.</li>
                        </ol>
                    </div>
                    <!-- Subheading: Setup Instructions -->
                    <h3 class="collapsible subheading">2.4 Setup Instructions</h3>
                    <div class="content">
                        <!-- Step 1: Review .env Files -->
                        <h4 style="margin-top: 0.5em;">Step 1: Review .env Files</h4>
                        <p>The <code>.env</code> files contain environment variables for each service (Elasticsearch, Kafka, Logstash, and the banking app). Ensure these files are properly configured:</p>
                        <ul>
                            <li><strong>Global .env file (root directory):</strong> Contains settings such as Elasticsearch ports, memory limits, and Kafka topics.</li>
                            <li><strong>Logstash .env file (under logstash/):</strong> Contains Logstash-specific configurations for Elasticsearch and Kafka.</li>
                            <li><strong>Banking app .env file (under banking/):</strong> Contains Kafka connection details and the topics for publishing transactions.</li>
                        </ul>
                        <!-- Step 2: Modify Environment Variables -->
                        <h4>Step 2: Modify Environment Variables (Optional)</h4>
                        <p>If required, customize the values in the <code>.env</code> files, such as ports, memory allocation, or Kafka topics.</p>
                        <p>For example, to change the Elasticsearch port:</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">ES_PORT=9200  # Default port for Elasticsearch</code></pre>
                        </div>
                        <!-- Step 3: Build the Docker Containers -->
                        <h4>Step 3: Build the Docker Containers</h4>
                        <p>The repository includes Dockerfiles for each service, and <code>docker-compose.yml</code> orchestrates them. To build the services, ensure Docker is running, and from the root directory, run:</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker-compose build</code></pre>
                        </div>
                        <img src="../../img/blog/pic1.png" alt="Docker Containers" class="image-container responsive-image" style="width: 65%;">
                    </div>
                    <!-- Subheading: Configuration -->
                    <h3 class="collapsible subheading">2.5 Configuration</h3>
                    <div class="content">
                        <p>The <code>docker-compose.yml</code> file defines the services, volumes, networks, and dependencies. It will start the following services:</p>
                        <ul>
                            <li>Elasticsearch (Coordination and Master Nodes)</li>
                            <li>Logstash</li>
                            <li>Kibana</li>
                            <li>Kafka</li>
                            <li>Zookeeper</li>
                            <li>Python Banking Application</li>
                        </ul>
                        <p><strong>Environment Variables:</strong></p>
                        <ul>
                            <li>All services are configured using values from <code>.env</code> files.</li>
                            <li>The variables ensure that Logstash consumes from the correct Kafka topics, Elasticsearch is properly secured, and the banking app connects to Kafka.</li>
                        </ul>
                    </div>
                    <!-- Subheading: Running Docker Containers -->
                    <h3 class="collapsible subheading">2.6 Running Docker Containers</h3>
                    <div class="content">
                        <p>After building the Docker containers, follow these steps to start the services:</p>
                        <!-- Start the Docker Stack -->
                        <h4 style="margin-top: 0.5em;">Step 1: Start the Docker Stack</h4>
                        <p>Run the following command to start the entire stack:</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker-compose up -d</code></pre>
                        </div>
                        <img src="../../img/blog/pic2.png" alt="Docker Containers" class="image-container responsive-image">
                        <!-- Step 2: Monitor the Logs -->
                        <h4>Step 2: Monitor the Logs</h4>
                        <p>To monitor the logs for any issues, run:</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker-compose logs -f</code></pre>
                        </div>
                        <!-- Step 3: Verify the Services -->
                        <h4>Step 3: Verify the Services</h4>
                        <p>To verify the services from the Docker host machine, check the following nodes:</p>
                        <ul class="custom-list">
                            <li>
                                <strong>es-coord:</strong>
                                <div>
                                    <img src="../../img/blog/pic3.png" alt="Docker Containers" class="image-container responsive-image">
                                </div>
                                <div>
                                    <img src="../../img/blog/pic4.png" alt="Docker Containers" class="image-container responsive-image">
                                </div>
                            </li>
                            <li>
                                <strong>es-masternode-01:</strong>
                                <div>
                                    <img src="../../img/blog/pic5.png" alt="Docker Containers" class="image-container responsive-image">
                                </div>
                                <div>
                                    <img src="../../img/blog/pic6.png" alt="Docker Containers" class="image-container responsive-image">
                                </div>
                            </li>
                            <li>
                                <strong>es-masternode-02:</strong>
                                <div>
                                    <img src="../../img/blog/pic7.png" alt="Docker Containers" class="image-container responsive-image">
                                </div>
                                <div>
                                    <img src="../../img/blog/pic8.png" alt="Docker Containers" class="image-container responsive-image">
                                </div>
                            </li>
                            <li style="margin-top: 2.5em;">
                                <strong>es-masternode-03:</strong>
                                <div>
                                    <img src="../../img/blog/pic9.png" alt="Docker Containers" class="image-container responsive-image">
                                </div>
                                <div>
                                    <img src="../../img/blog/pic10.png" alt="Docker Containers" class="image-container responsive-image">
                                </div>
                            </li>
                            <li style="margin-top: 2.5em;">
                                <strong>kibana-01:</strong>
                                <div>
                                    <img src="../../img/blog/pic11.png" alt="Docker Containers" class="image-container responsive-image">
                                </div>
                                <div>
                                    <img src="../../img/blog/pic12.png" alt="Docker Containers" class="image-container responsive-image">
                                </div>
                            </li>
                            <li style="margin-top: 2.5em;">
                                <strong>kibana-02:</strong>
                                <div>
                                    <img src="../../img/blog/pic13.png" alt="Docker Containers" class="image-container responsive-image">
                                </div>
                                <div>
                                    <img src="../../img/blog/pic14.png" alt="Docker Containers" class="image-container responsive-image">
                                </div>
                            </li>
                        </ul>
                    </div>
                    <!-- Subheading: Troubleshooting and Common Issues -->
                    <h3 class="collapsible subheading">2.7 Troubleshooting and Common Issues</h3>
                    <div class="content">
                        <!-- Issue 1: Incorrect Line Endings (Windows) -->
                        <h4 style="margin-top: 0.5em;"><strong>Issue 1:</strong> Incorrect Line Endings (If on Windows)</h4>
                        <p>When using Windows, the files may have Windows-style line endings (CRLF), which are incompatible with Linux-based Docker containers that expect Unix-style line endings (LF). This can cause the file to fail to execute properly, resulting in errors like no such file or directory.</p>
                        <strong>Solution:</strong>
                        <p>Convert the file to use Unix-style line endings (LF):</p>
                        <ol>
                            <li>
                                <strong>Using Git Configuration:</strong>
                                <p>Run the following command to ensure Git converts line endings to LF when checking out files:</p>
                                <div class="code-block">
                                    <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                                    <pre><code class="language-bash">git config --global core.autocrlf input</code></pre>
                                </div>
                            </li>
                            <li>
                                <strong>Using dos2unix Tool:</strong>
                                <p>If you have the dos2unix utility installed, convert the line endings by running:</p>
                                <div class="code-block">
                                    <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                                    <pre><code class="language-bash">dos2unix logstash/entrypoint.sh</code></pre>
                                </div>
                            </li>
                        </ol>
                        <p>After converting the line endings, rebuild the Docker image using:</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker build --no-cache -f Dockerfile .</code></pre>
                        </div>
                        <!-- Issue 2: Docker Build Fails -->
                        <h4><strong>Issue 2:</strong> Docker Build Fails</h4>
                        <p><strong>Solution:</strong> Check for errors in the <strong><i>Dockerfile</i></strong>, such as missing dependencies. Ensure Docker is properly installed and the internet connection is stable for downloading required packages.</p>
                        <!-- Issue 3: Kafka Connection Issues -->
                        <h4><strong>Issue 3:</strong> Kafka Connection Issues</h4>
                        <p><strong>Solution:</strong> Ensure Kafka is running on the correct port (default 9092) and that it is accessible from other containers. Use netcat or ping from within the containers to check connectivity:</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker exec -it &lt;container_id&gt; ping kafka</code></pre>
                        </div>
                        <!-- Issue 4: Logstash Not Processing Data -->
                        <h4><strong>Issue 4:</strong> Logstash Not Processing Data</h4>
                        <p><strong>Solution:</strong> Check Logstash logs:</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker-compose logs logstash</code></pre>
                        </div>
                        <p>Ensure the Kafka topics are correctly set in the <code>.env</code> file and that the <code>logstash.template.conf</code> file references them.</p>
                        <!-- Issue 5: Memory Limits Exceeded -->
                        <h4><strong>Issue 5:</strong> Memory Limits Exceeded</h4>
                        <p><strong>Solution:</strong> Increase memory allocation for Elasticsearch, Logstash, or Kibana in the <code>.env</code> file:</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">ES_MEM_LIMIT=4294967296  # 4 GB for Elasticsearch</code></pre>
                        </div>
                    </div>
                </div>
            </section>
            <!-- 3. Docker Commands -->
            <section id="docker-commands" class="blog-section">
                <h2 class="collapsible main-heading">3. Docker Commands</h2>
                <div class="content">
                    <h3 class="collapsible subheading">3.1 Docker System Management Commands</h3>
                    <div class="content">
                        <h4 style="margin-top: 0em;">docker version</h4>
                        <p>This command shows the installed version of Docker, including both the client and server versions. It is useful for verifying the Docker installation and ensuring compatibility with specific features.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker version</code></pre>
                        </div>
                        <h4>docker info</h4>
                        <p>Provides detailed information about the Docker environment, including the number of containers, images, networks, storage drivers, and other system details. This is useful for diagnosing system issues or understanding the current Docker setup.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker info</code></pre>
                        </div>
                        <h4>docker system prune</h4>
                        <p>Cleans up unused containers, networks, images, and volumes. This command helps reclaim disk space by removing resources that are no longer in use.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker system prune</code></pre>
                        </div>
                    </div>
                    <h3 class="collapsible subheading">3.2 Docker Container Management Commands</h3>
                    <div class="content">
                        <h4 style="margin-top: 0em;">docker ps</h4>
                        <p>Lists all running containers, displaying details such as container IDs, names, and the image they are running. It is helpful for monitoring active containers.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker ps</code></pre>
                        </div>
                        <h4>docker ps -a</h4>
                        <p>Lists all containers, including stopped ones. This command helps in tracking containers that have been created but are not currently running.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker ps -a</code></pre>
                        </div>
                        <h4>docker start &lt;container-name-or-id&gt;</h4>
                        <p>Starts a stopped container. Use this command when you want to resume a container without creating a new one.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker start &lt;container-name-or-id&gt;</code></pre>
                        </div>
                        <h4>docker stop &lt;container-name-or-id&gt;</h4>
                        <p>Stops a running container gracefully. It allows the container to terminate its processes cleanly before shutting down.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker stop &lt;container-name-or-id&gt;</code></pre>
                        </div>
                        <h4>docker restart &lt;container-name-or-id&gt;</h4>
                        <p>Restarts a running or stopped container. This command is useful for applying new configurations or refreshing a container without removing it.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker restart &lt;container-name-or-id&gt;</code></pre>
                        </div>
                        <h4>docker rm &lt;container-name-or-id&gt;</h4>
                        <p>Removes a stopped container permanently. It clears the container's associated resources but doesn't affect the image used to create it.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker rm &lt;container-name-or-id&gt;</code></pre>
                        </div>
                        <h4>docker run -d --name &lt;container-name&gt; &lt;image-name&gt;</h4>
                        <p>Runs a new container from the specified image in detached mode (-d), which means the container runs in the background. The --name flag assigns a custom name to the container for easier identification.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker run -d --name &lt;my_container&gt; &lt;my_image&gt;</code></pre>
                        </div>
                    </div>
                    <h3 class="collapsible subheading">3.3 Docker Image Management Commands</h3>
                    <div class="content">
                        <h4 style="margin-top: 0em;">docker images</h4>
                        <p>Lists all Docker images available on the local machine. It provides details like the repository name, tag, and size of each image.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker images</code></pre>
                        </div>
                        <h4>docker rmi &lt;image-name-or-id&gt;</h4>
                        <p>Removes an image from the local Docker registry. This command helps in cleaning up unused or outdated images.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker rmi &lt;image-name-or-id&gt;</code></pre>
                        </div>
                        <h4>docker build -t &lt;image-name&gt; &lt;path-to-dockerfile&gt;</h4>
                        <p>Builds a Docker image from a <strong><i>Dockerfile</i></strong> and tags it with a custom name (-t).</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker build -t my_image .</code></pre>
                        </div>
                        <h4>docker pull &lt;image-name&gt;</h4>
                        <p>Downloads an image from a Docker registry like Docker Hub.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker pull my_image</code></pre>
                        </div>
                        <h4>docker push &lt;image-name&gt;</h4>
                        <p>Uploads a locally built image to a Docker registry for sharing or deployment.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker push my_image</code></pre>
                        </div>
                    </div>
                    <h3 class="collapsible subheading">3.4 Docker Network Management Commands</h3>
                    <div class="content">
                        <h4 style="margin-top: 0em;">docker network ls</h4>
                        <p>Lists all Docker networks on the host, showing network names, IDs, and types (bridge, overlay, etc.).</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker network ls</code></pre>
                        </div>
                        <h4>docker network inspect &lt;network-name&gt;</h4>
                        <p>Provides detailed information about a specific Docker network, including connected containers and settings.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker network inspect my_network</code></pre>
                        </div>
                        <h4>docker network create &lt;network-name&gt;</h4>
                        <p>Creates a new custom Docker network for isolating containers.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker network create my_network</code></pre>
                        </div>
                        <h4>docker network rm &lt;network-name&gt;</h4>
                        <p>Removes a Docker network that is no longer in use.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker network rm my_network</code></pre>
                        </div>
                        <h4>docker network prune</h4>
                        <p>Removes all unused networks for cleanup.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker network prune</code></pre>
                        </div>
                        <h4>docker run --network host &lt;image&gt;</h4>
                        <p>Runs a container on the host network, giving it access to the host's network interface.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker run --network host my_image</code></pre>
                        </div>
                        <h4>docker run --network none &lt;image&gt;</h4>
                        <p>Runs a container with no network access, used for security purposes.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker run --network none my_image</code></pre>
                        </div>
                    </div>
                    <h3 class="collapsible subheading">3.5 Docker Log Management Commands</h3>
                    <div class="content">
                        <h4 style="margin-top: 0em;">docker logs &lt;container-name-or-id&gt;</h4>
                        <p>Displays logs generated by a container. It is useful for debugging and monitoring container activity.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker logs my_container</code></pre>
                        </div>
                        <h4>docker logs -f &lt;container-name-or-id&gt;</h4>
                        <p>Follows the log output in real-time. This is particularly useful for monitoring long-running processes.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker logs -f my_container</code></pre>
                        </div>
                        <h4>docker logs --since &lt;timestamp&gt; &lt;container-name-or-id&gt;</h4>
                        <p>Shows logs generated by the container since the specified timestamp. This helps focus on recent logs, especially after a specific event.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker logs --since 2023-09-01T00:00:00Z my_container</code></pre>
                        </div>
                        <h4>docker logs --until &lt;timestamp&gt; &lt;container-name-or-id&gt;</h4>
                        <p>Shows logs generated up to a specified timestamp. This is useful for investigating logs within a specific time range.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker logs --until 2023-09-01T00:00:00Z my_container</code></pre>
                        </div>
                        <h4>docker logs --tail &lt;number-of-lines&gt; &lt;container-name-or-id&gt;</h4>
                        <p>Displays the last N lines of logs from the container. This is useful for quickly reviewing the latest activity.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker logs --tail 50 my_container</code></pre>
                        </div>
                        <h4>docker logs -t &lt;container-name-or-id&gt;</h4>
                        <p>Shows logs with timestamps, which is useful for tracking when specific events occurred.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker logs -t my_container</code></pre>
                        </div>
                        <h4>docker logs --details &lt;container-name-or-id&gt;</h4>
                        <p>Displays extra details about the log messages, such as environment variables and labels, if available.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker logs --details my_container</code></pre>
                        </div>
                        <h4>docker logs -f -t --tail 100 &lt;container-name-or-id&gt;</h4>
                        <p>Combines multiple log options: follows real-time logs, includes timestamps, and shows the last 100 log lines.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker logs -f -t --tail 100 my_container</code></pre>
                        </div>
                    </div>
                    <h3 class="collapsible subheading">3.6 Docker Volume Management Commands</h3>
                    <div class="content">
                        <h4 style="margin-top: 0em;">docker volume ls</h4>
                        <p>Lists all Docker volumes on the host. It shows volume names and helps identify which volumes are in use.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker volume ls</code></pre>
                        </div>
                        <h4>docker volume create &lt;volume-name&gt;</h4>
                        <p>Creates a new Docker volume. Volumes are used for persisting data across container restarts.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker volume create my_volume</code></pre>
                        </div>
                        <h4>docker volume inspect &lt;volume-name&gt;</h4>
                        <p>Displays detailed information about a specific volume, including its mount point and usage.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker volume inspect my_volume</code></pre>
                        </div>
                        <h4>docker volume rm &lt;volume-name&gt;</h4>
                        <p>Removes a Docker volume that is no longer needed. This command is useful for cleaning up unused volumes.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker volume rm my_volume</code></pre>
                        </div>
                    </div>
                    <h3 class="collapsible subheading">3.7 Docker Exec (Running Commands in Containers)</h3>
                    <div class="content">
                        <h4 style="margin-top: 0em;">docker exec &lt;container-name-or-id&gt; &lt;my_command&gt;</h4>
                        <p>Executes a command inside a running container. It is commonly used to run commands like checking the status of services within the container.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker exec &lt;my_container&gt; ls -l</code></pre>
                        </div>
                        <h4>docker exec -it &lt;container-name-or-id&gt; /bin/bash</h4>
                        <p>Opens an interactive shell session inside a running container. This is useful for inspecting and troubleshooting the container from within.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker exec -it &lt;my_container&gt; /bin/bash</code></pre>
                        </div>
                        <h4>docker exec -d &lt;container-name-or-id&gt; &lt;my_command&gt;</h4>
                        <p>Runs a command inside a container in detached mode, meaning the command runs in the background.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker exec -d &lt;my_container&gt; &lt;my_command&gt;</code></pre>
                        </div>
                        <h4>docker exec -e VAR_NAME=value &lt;container-name-or-id&gt; &lt;my_command&gt;</h4>
                        <p>Executes a command inside a container with a specified environment variable. This is useful for temporarily passing environment variables into a running container.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker exec -e VAR_NAME=value &lt;my_container&gt; &lt;my_command&gt;</code></pre>
                        </div>
                        <h4>docker exec -u &lt;user&gt; &lt;container-name-or-id&gt; &lt;my_command&gt;</h4>
                        <p>Executes a command inside a container as a specific user. This is useful when dealing with permission-sensitive operations.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker exec -u &lt;user&gt; &lt;my_container&gt; &lt;my_command&gt;</code></pre>
                        </div>
                        <h4>docker exec -w &lt;directory_path&gt; &lt;container-name-or-id&gt; &lt;my_command&gt;</h4>
                        <p>Runs a command inside a container with a specified working directory. This ensures the command is executed from the correct location within the container.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker exec -w &lt;directory_path&gt; &lt;my_container&gt; &lt;my_command&gt;</code></pre>
                        </div>
                    </div>
                    <h3 class="collapsible subheading">3.8 Docker Networking Commands</h3>
                    <div class="content">
                        <h4 style="margin-top: 0em;">docker network ls</h4>
                        <p>Lists all Docker networks on the host. This command helps identify networks created and their types.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker network ls</code></pre>
                        </div>
                        <h4>docker network create &lt;network-name&gt;</h4>
                        <p>Creates a new Docker network for containers to communicate. This is useful for managing container communication.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker network create my_network</code></pre>
                        </div>
                        <h4>docker network inspect &lt;network-name&gt;</h4>
                        <p>Displays detailed information about a specific network, including connected containers and configuration settings.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker network inspect my_network</code></pre>
                        </div>
                        <h4>docker network connect &lt;network-name&gt; &lt;container-name-or-id&gt;</h4>
                        <p>Connects a running container to a specified network. This is useful for changing a container's networking environment.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker network connect my_network my_container</code></pre>
                        </div>
                        <h4>docker network disconnect &lt;network-name&gt; &lt;container-name-or-id&gt;</h4>
                        <p>Disconnects a container from a specified network. This helps isolate containers as needed.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker network disconnect my_network my_container</code></pre>
                        </div>
                    </div>
                    <h3 class="collapsible subheading">3.9 Docker Compose Commands</h3>
                    <div class="content">
                        <h4 style="margin-top: 0em;">docker-compose up</h4>
                        <p>Starts all containers defined in the <strong><i>docker-compose.yml</i></strong> file. It creates networks and volumes as specified in the file.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker-compose up</code></pre>
                        </div>
                        <h4>docker-compose down</h4>
                        <p>Stops and removes all containers defined in the <strong><i>docker-compose.yml</i></strong> file, along with networks and volumes if specified.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker-compose down</code></pre>
                        </div>
                        <h4>docker-compose ps</h4>
                        <p>Lists the containers that are part of the application defined in the <strong><i>docker-compose.yml</i></strong> file, along with their status.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker-compose ps</code></pre>
                        </div>
                        <h4>docker-compose logs</h4>
                        <p>Displays the logs of all containers defined in the <strong><i>docker-compose.yml</i></strong> file, useful for debugging.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker-compose logs</code></pre>
                        </div>
                        <h4>docker-compose build</h4>
                        <p>Builds or rebuilds services defined in the <strong><i>docker-compose.yml</i></strong> file. This is essential when changes are made to the <strong><i>Dockerfile</i></strong>.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker-compose build</code></pre>
                        </div>
                    </div>
                    <h3 class="collapsible subheading">3.10 Docker Swarm Commands</h3>
                    <div class="content">
                        <h4 style="margin-top: 0em;">docker swarm init</h4>
                        <p>Initializes a new Swarm cluster, making the current host a manager. It is the first step to setting up a Swarm.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker swarm init</code></pre>
                        </div>
                        <h4>docker swarm join &lt;manager-ip&gt;:2377</h4>
                        <p>Joins a worker node to a Swarm cluster, enabling it to participate in distributed services.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker swarm join 192.168.1.100:2377</code></pre>
                        </div>
                        <h4>docker service create &lt;options&gt; &lt;image&gt;</h4>
                        <p>Creates a new service in the Swarm, allowing you to scale applications across multiple nodes.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker service create --replicas 3 --name my_service nginx</code></pre>
                        </div>
                        <h4>docker service ls</h4>
                        <p>Lists all services running in the Swarm cluster. It shows the number of replicas and their status.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker service ls</code></pre>
                        </div>
                        <h4>docker stack deploy -c &lt;file.yml&gt; &lt;stack-name&gt;</h4>
                        <p>Deploys a stack (a group of services) defined in a YAML file, allowing for easier management of multi-service applications.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-bash">docker stack deploy -c docker-compose.yml my_stack</code></pre>
                        </div>
                    </div>
                </div>
            </section>
            <!-- 4. Containers -->
            <section id="containers" class="blog-section">
                <h2 class="collapsible main-heading">4. Containers</h2>
                <div class="content">
                    <p style="margin-top: 0.5em;">Containers are the building blocks of this architecture. Each service (Elasticsearch, Logstash, Kafka, etc.) runs inside its own Docker container. These containers are lightweight, isolated environments that allow the services to run independently on the same host machine. The key Docker containers in this architecture are:</p>
                    <h8 style="margin-top: 0.5em;"><strong>Elasticsearch Cluster:</strong></h8>
                    <p>A distributed search and analytics engine. It stores all the ingested data and provides search capabilities.</p>
                    <ul>
                        <li>Coordination Node (1 container)</li>
                        <li>Master Nodes (3 containers)</li>
                    </ul>
                    <h8 style="margin-top: 0.5em;"><strong>Kibana Instances:</strong></h8>
                    <p>A data visualization dashboard for Elasticsearch. It allows users to create and share dynamic dashboards displaying changes to Elasticsearch queries in real-time.</p>
                    <ul>
                        <li>Two Kibana containers for dashboard and data visualization.</li>
                    </ul>
                    <h8 style="margin-top: 0.5em;"><strong>Logstash:</strong></h8>
                    <p>A server-side data processing pipeline that ingests data from multiple sources simultaneously, transforms it, and sends it to your favorite "stash" (like Elasticsearch).</p>
                    <ul>
                        <li>One container that connects Kafka (producer) to Elasticsearch (consumer).</li>
                    </ul>
                    <h8 style="margin-top: 0.5em;"><strong>Zookeeper:</strong></h8>
                    <p>A centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. It is used to manage Kafka brokers.</p>
                    <ul>
                        <li>Zookeeper (1 container): Manages Kafka brokers.</li>
                    </ul>
                    <h8 style="margin-top: 0.5em;"><strong>Kafka:</strong></h8>
                    <p>A distributed event streaming platform capable of handling trillions of events a day. It acts as a message broker that allows the banking app to publish messages.</p>
                    <ul>
                        <li>Kafka (1 container): Receives and stores data from the banking app.</li>
                    </ul>
                    <h8 style="margin-top: 0.5em;"><strong>Python Banking App:</strong></h8>
                    <p>A Python application that simulates bank transactions and pushes data to Kafka for further processing.</p>
                    <ul>
                        <li>One container that generates fake bank transactions and sends them to Kafka.</li>
                    </ul>
                </div>
            </section>
            <!-- 5. Network Config -->
            <section id="network-config" class="blog-section">
                <h2 class="collapsible main-heading">5. Network Configuration</h2>
                <div class="content">
                    <p style="margin-top: 0.5em;">Docker allows the creation of bridge networks to isolate and control communication between containers. Two bridge networks are defined in this architecture:</p>
                    <h8 style="margin-top: 0.5em;"><strong>elastic_network:</strong></h8>
                    <p>This network connects all the ELK containers (Elasticsearch, Logstash, Kibana). Its purpose is to ensure isolated communication for the ELK stack and allows the following:</p>
                    <ul>
                        <li>Kibana instances connect to the Elasticsearch coordination node.</li>
                        <li>Logstash connects to Elasticsearch to send processed data.</li>
                    </ul>
                    <h8 style="margin-top: 0.5em;"><strong>kafka_network:</strong></h8>
                    <p>This network connects the Kafka stack (Kafka and Zookeeper) and the Python banking application. The purpose is to enable communication between:</p>
                    <ul>
                        <li>Kafka and Zookeeper for managing Kafka brokers.</li>
                        <li>The Python banking application and Kafka for publishing data to the Kafka topic.</li>
                    </ul>
                    <p>Logstash is connected to both networks (<strong><i>elastic_network</i></strong> and <strong><i>kafka_network</i></strong>) to act as a bridge, facilitating data transfer between Kafka and Elasticsearch.</p>
                </div>
            </section>
            <!-- 6. Host Machine -->
            <section id="host-machine" class="blog-section">
                <h2 class="collapsible main-heading">6. Host Machine</h2>
                <div class="content">
                    <p>The host machine is the physical or virtual machine where Docker is installed. This machine orchestrates all the Docker containers and provides the necessary resources (CPU, memory, disk space) for the containers to operate.</p>
                    <h8 style="margin-top: 0.5em;"><strong>Docker Engine:</strong></h8>
                    <p>The Docker engine runs on the host machine, allowing containers to run independently with isolated resources.</p>
                    <h8 style="margin-top: 0.5em;"><strong>Networks on the Host:</strong></h8>
                    <p>The Docker bridge networks (<strong><i>elastic_network</i></strong>, <strong><i>kafka_network</i></strong>) exist only within the Docker environment but use the host machine's network interfaces for external communication.</p>
                </div>
            </section>
            <!-- 7. Architecture -->
            <section id="architecture" class="blog-section">
                <h2 class="collapsible main-heading">7. Architecture</h2>
                <div class="content">
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">7.1 Elasticsearch Nodes</h3>
                    <div class="content">
                        <p style="margin-top: 0.5em;">The Elasticsearch cluster comprises different containers, each playing a crucial role in managing, indexing, and storing data.</p>
                        <h8 style="margin-top: 0.5em;"><strong>Coordination Node (1):</strong></h8>
                        <ul>
                            <li>Acts as a gateway for communication between Kibana, Logstash, and Elasticsearch master nodes.</li>
                            <li>Routes and distributes requests from Kibana and Logstash to the appropriate master nodes.</li>
                            <li>Manages traffic and client requests, but does not store data.</li>
                            <li>Ensures efficient coordination of data distribution across the master nodes.</li>
                        </ul>
                        <h8 style="margin-top: 0.5em;"><strong>Master Nodes (3):</strong></h8>
                        <ul>
                            <li>Responsible for cluster state management, data indexing, and search operations.</li>
                            <li>Distribute and replicate data to ensure high availability and redundancy within the cluster.</li>
                            <li>Manage indexing and searching tasks, ensuring consistent performance across the cluster.</li>
                        </ul>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">7.2 Kibana Instances</h3>
                    <div class="content">
                        <ul>
                            <li>Provides a user interface for visualizing and interacting with data stored in Elasticsearch.</li>
                            <li>Both Kibana containers connect to the coordination node for data retrieval.</li>
                            <li>Enable users to create dashboards, visualizations, and queries based on indexed data in Elasticsearch.</li>
                            <li>Serve as the front-end for Elasticsearch, offering real-time data insights.</li>
                        </ul>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">7.3 Logstash</h3>
                    <div class="content">
                        <p style="margin-top: 0.5em;">Logstash is the key component for data ingestion in the pipeline, bridging Kafka and Elasticsearch.</p>
                        <ul>
                            <li>Consumes data from the Kafka topic (<strong><i>banking_transactions</i></strong>) generated by the Python banking app.</li>
                            <li>Processes and forwards the data to Elasticsearch for indexing.</li>
                            <li>Acts as a bridge between two Docker networks (<strong><i>elastic_network</i></strong> and <strong><i>kafka_network</i></strong>), enabling seamless data transfer between Kafka and Elasticsearch.</li>
                            <li>Configured via a <strong><i>logstash.conf</i></strong> file, where Kafka is the input source and Elasticsearch is the output destination.</li>
                            <ul>
                                <li><strong>Input:</strong> Reads transaction data from Kafka.</li>
                                <li><strong>Output:</strong> Processes and sends the data to Elasticsearch for storage and indexing.</li>
                            </ul>
                        </ul>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">7.4 Zookeeper and Kafka</h3>
                    <div class="content">
                        <p style="margin-top: 0.5em;">Zookeeper and Kafka form the backbone of the message queue system, which manages the real-time data flow.</p>
                        <h8 style="margin-top: 0.5em;"><strong>Zookeeper:</strong></h8>
                        <ul>
                            <li>Coordinates Kafka brokers, ensuring the proper functioning of the Kafka cluster.</li>
                            <li>Manages cluster configuration, maintaining a consistent view of the brokers and ensuring they operate correctly.</li>
                        </ul>
                        <h8 style="margin-top: 0.5em;"><strong>Kafka:</strong></h8>
                        <ul>
                            <li>Acts as the message broker, receiving transaction data from the banking app and making it available to consumers like Logstash.</li>
                            <li>Data is published to a Kafka topic (<strong><i>banking_transactions</i></strong>), where it is stored until consumed by Logstash.</li>
                            <li>Brokers in Kafka manage and handle message distribution, ensuring that data from the banking app is available for downstream processing.</li>
                        </ul>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">7.5 Python Banking Application</h3>
                    <div class="content">
                        <p style="margin-top: 0.5em;">The Python-based banking app simulates a data producer that generates synthetic bank transactions and sends them to Kafka.</p>
                        <ul>
                            <li>Generates fake transaction data, including fields like transaction_id, amount, and timestamp.</li>
                            <li>Publishes the data to the Kafka topic (<strong><i>banking_transactions</i></strong>), serving as the producer in the data pipeline.</li>
                            <li>Connected to the <strong><i>kafka_network</i></strong>, allowing direct communication with the Kafka brokers for data transfer.</li>
                            <li>Acts as the source of transaction data in the pipeline, simulating real-time financial transactions for testing purposes.</li>
                        </ul>
                    </div>
                    <div>
                        <img src="../../img/blog/pic15.png" alt="Architecture" class="image-container responsive-image" style="width: 80%;">
                    </div>
                </div>
            </section>
            <!-- 8. Data Flow -->
            <section id="data-flow" class="blog-section">
                <h2 class="collapsible main-heading">8. Data Flow</h2>
                <div class="content">
                    <p style="margin-top: 0.5em;">The architecture comprises various containers (Python banking app, Kafka, Logstash, Elasticsearch, and Kibana), each playing a specific role in the data pipeline. Here's a step-by-step breakdown of the data flow between these containers, highlighting how data is generated, ingested, processed, stored, and visualized.</p>
                    <div>
                        <img src="../../img/blog/pic16.png" alt="Data Flow" class="image-container responsive-image" style="width: 80%;">
                    </div>
                    <br>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">8.1 Data Generation</h3>
                    <div class="content">
                        <h8 style="margin-top: 0.5em;"><strong>Python Banking Application:</strong></h8>
                        <ul>
                            <li>The Python banking app generates random synthetic transaction data. This data simulates real-world bank transactions and includes key details such as:</li>
                            <ul>
                                <li>transaction_id: A unique identifier for each transaction.</li>
                                <li>amount: A randomly generated amount for each transaction.</li>
                                <li>transaction_timestamp: The time the transaction occurred.</li>
                            </ul>
                            <li>The generated data is published to Kafka over the <strong><i>kafka_network</i></strong>. This network is shared between Kafka, Zookeeper, and the Python banking app, enabling communication between these components.</li>
                            <li>Publishing to Kafka: The transaction data is sent to a Kafka topic named <strong><i>banking_transactions</i></strong>. Kafka topics are essentially channels where producers (like the banking app) send data and consumers (like Logstash) subscribe to receive it.</li>
                        </ul>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">8.2 Data Ingestion</h3>
                    <div class="content">
                        <h8 style="margin-top: 0.5em;"><strong>Kafka:</strong></h8>
                        <ul>
                            <li>Kafka brokers, managed by Zookeeper, receive the transaction data from the banking app. The data is stored in the Kafka topic <strong><i>banking_transactions</i></strong>.</li>
                            <li>Kafka ensures the message (transaction data) is persisted until a consumer (Logstash) subscribes to the topic and processes the data.</li>
                            <li>Kafka acts as a buffer to store data until Logstash is ready to consume it, ensuring a reliable and fault-tolerant message queue.</li>
                        </ul>
                        <h8 style="margin-top: 0.5em;"><strong>Logstash:</strong></h8>
                        <ul>
                            <li>Logstash is connected to both the <strong><i>kafka_network</i></strong> (to consume data from Kafka) and the <strong><i>elastic_network</i></strong> (to forward data to Elasticsearch).</li>
                            <li>Logstash is configured to subscribe to the <strong><i>banking_transactions</i></strong> topic in Kafka. Once connected, it continuously listens for new transaction data produced by the banking app.</li>
                            <li>As new data arrives in Kafka, Logstash consumes the data in near real-time, moving it to the next stage of the pipeline for further processing.</li>
                        </ul>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">8.3 Data Processing</h3>
                    <div class="content">
                        <h8 style="margin-top: 0.5em;"><strong>Logstash Processing:</strong></h8>
                        <ul>
                            <li>After consuming the transaction data from Kafka, Logstash processes the data according to its configuration file (<strong><i>logstash.conf</i></strong>). This step might include:</li>
                            <ul>
                                <li>Data transformation: Logstash can transform or modify the data (e.g., converting field names, adjusting formats).</li>
                                <li>Filtering: Specific filters can be applied to clean, parse, or structure the data. For example, Logstash could apply grok patterns to extract specific fields from raw data or standardize data formats.</li>
                            </ul>
                            <li>The processing ensures that the data is in a format suitable for indexing in Elasticsearch. This structured format will make searching, analyzing, and visualizing the data more efficient.</li>
                        </ul>
                        <h8 style="margin-top: 0.5em;"><strong>Forwarding to Elasticsearch:</strong></h8>
                        <ul>
                            <li>Once the data is processed, Logstash forwards the processed data to Elasticsearch for storage. This is done through the <strong><i>elastic_network</i></strong>, which connects Logstash to the Elasticsearch containers.</li>
                            <li>Logstash Output Configuration: Logstash’s output configuration specifies the destination (Elasticsearch coordination node) where the processed data should be sent for indexing.</li>
                        </ul>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">8.4 Data Storage</h3>
                    <div class="content">
                        <h8 style="margin-top: 0.5em;"><strong>Elasticsearch Coordination Node:</strong></h8>
                        <ul>
                            <li>The coordination node in Elasticsearch is responsible for receiving the data from Logstash. It acts as an intermediary between Logstash and the Elasticsearch master nodes.</li>
                            <li>Upon receiving the data, the coordination node distributes the indexing requests to the appropriate master nodes in the Elasticsearch cluster.</li>
                        </ul>
                        <h8 style="margin-top: 0.5em;"><strong>Master Nodes:</strong></h8>
                        <ul>
                            <li>The master nodes handle the indexing and storage of the transaction data. Elasticsearch breaks down the data into indices (structured records) to enable fast search and retrieval.</li>
                            <li>The data is replicated and distributed across multiple master nodes, ensuring high availability and fault tolerance. This means even if one node goes down, the data remains accessible from another node.</li>
                            <li>Cluster Management: The master nodes ensure that the cluster remains healthy, balancing the load across all nodes for optimal performance.</li>
                        </ul>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">8.5 Data Visualization</h3>
                    <div class="content">
                        <h8 style="margin-top: 0.5em;"><strong>Kibana:</strong></h8>
                        <ul>
                            <li>Kibana connects to the Elasticsearch coordination node to access the stored transaction data. The coordination node provides Kibana with the necessary information to query the Elasticsearch cluster.</li>
                            <li>Retrieving Data: Kibana retrieves data from Elasticsearch through queries, allowing users to search and filter the transaction records based on different criteria (e.g., transaction amount, time, or transaction ID).</li>
                        </ul>
                        <h8 style="margin-top: 0.5em;"><strong>Dashboards and Visualizations:</strong></h8>
                        <ul>
                            <li>Users can use Kibana to build dashboards, graphs, charts, and visualizations based on the stored transaction data. These visualizations provide insights into patterns and trends in the financial data.</li>
                            <li>Kibana's interface supports creating real-time visualizations, enabling users to see live updates as new data is ingested into Elasticsearch.</li>
                            <li>User Interaction: Through Kibana, users can explore the transaction data, create custom queries, and display real-time data in the form of visual dashboards, offering insights into financial transactions.</li>
                        </ul>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">8.6 Data Flow Summary</h3>
                    <div class="content">
                        <ol>
                            <li>Data Generation: The Python banking app generates fake transaction data and publishes it to the Kafka topic (<strong><i>banking_transactions</i></strong>) over the <strong><i>kafka_network</i></strong>.</li>
                            <li>Data Ingestion: Kafka stores the data in the topic until Logstash subscribes to and consumes it from the Kafka message queue.</li>
                            <li>Data Processing: Logstash processes the consumed data, applying transformations or filters, and forwards the cleaned data to Elasticsearch for indexing through the <strong><i>elastic_network</i></strong>.</li>
                            <li>Data Storage: Elasticsearch stores the processed transaction data across its master nodes, ensuring data replication and high availability.</li>
                            <li>Data Visualization: Kibana connects to Elasticsearch, allowing users to query and visualize the stored transaction data through custom dashboards and visual reports.</li>
                        </ol>
                        <p>This data flow outlines a real-time streaming data pipeline that efficiently generates, processes, stores, and visualizes data using Docker containers and networked services.</p>
                    </div>
                </div>
            </section>
            <!-- 9. Directory Structure -->
            <section id="directory-structure" class="blog-section">
                <h2 class="collapsible main-heading">9. Directory Structure</h2>
                <div class="content">
                    <p>The project directory <strong>(/project-directory)</strong> is structured to organize all the necessary files and configurations required for deploying and managing the multi-container ELK stack with Kafka and a Python-based banking app. Each subdirectory contains relevant files for individual services, such as Docker configuration files, environment variables, and service-specific scripts. Each file within the structure plays a crucial role in configuring, deploying, and managing the containers in this setup. Further details of each file are explained below.</p>
                    <div class="code-block">
                        <pre>
<code class="language-bash">/project-directory
│
├── docker-compose.yml
├── .env
│
├── banking/
│    ├── .env
│    ├── banking_app.template.py
│    ├── Dockerfile
│    ├── entrypoint.sh
│    └── requirements.txt
│
└── logstash/
     ├── .env
     ├── Dockerfile
     ├── entrypoint.sh
     └── logstash.template.conf
</code></pre>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">9.1 docker-compose.yml</h3>
                    <div class="content">
                        <h5 style="margin-top: 0.1em;"><strong>docker version</strong></h5>
                        <p>This <strong><i>docker-compose.yml</i></strong> file defines the multi-container setup for the ELK stack (Elasticsearch, Logstash, Kibana) integrated with Kafka, Zookeeper, and a Python-based banking application. The configuration specifies how the services interact with each other, defines network topologies, and ensures that volumes are used for persistent storage. Below is a detailed explanation of each section:</p>
                        <h5><strong>Volumes</strong></h5>
                        <p>These volumes store persistent data for various services, ensuring that data is retained even when containers are restarted.</p>
                        <ul>
                            <li><strong>certs:</strong> Stores SSL/TLS certificates used by Elasticsearch and Kibana to secure communication.</li>
                            <li><strong>esdata-coord:</strong> Stores data for the Elasticsearch coordination node.</li>
                            <li><strong>es-data-01, es-data-02, es-data-03:</strong> Store data for the respective Elasticsearch master nodes.</li>
                            <li><strong>kibana-data-01, kibana-data-02:</strong> Store Kibana configuration data.</li>
                        </ul>
                        <p>These volumes ensure data is not lost between container restarts and support the persistence of Elasticsearch indices, Kibana settings, and certificates.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">volumes:
    certs:
        driver: local
    esdata-coord:
        driver: local
    es-data-01:
        driver: local
    es-data-02:
        driver: local
    es-data-03:
        driver: local
    kibana-data-01:
        driver: local
    kibana-data-02:
        driver: local
</code></pre>
                        </div>
                        <h5><strong>Networks</strong></h5>
                        <p>Two custom bridge networks are created to isolate the ELK stack and Kafka services.</p>
                        <ul>
                            <li><strong>elastic_network:</strong> Connects the ELK stack (Elasticsearch, Logstash, Kibana) containers.</li>
                            <li><strong>kafka_network:</strong> Connects Kafka, Zookeeper, and the Python banking app.</li>
                        </ul>
                        <p>The use of separate networks ensures proper service isolation while allowing containers to communicate across networks (e.g., Logstash needs to access both Kafka and Elasticsearch).</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">networks:
    elastic_network:
        driver: bridge
    kafka_network:
        driver: bridge
</code></pre>
                        </div>
                        <h5><strong>Services</strong></h5>
                        <h6 style="color: #149ddd; margin-top: 0.1em;"><strong>Setup Service</strong></h6>
                        <ul>
                            <li><strong>Purpose:</strong> Initializes the environment by generating SSL certificates for Elasticsearch and Kibana, handling security configurations.</li>
                            <li><strong>Image:</strong> <i>docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}</i> — The official Elasticsearch image with a dynamic version.</li>
                            <li><strong>Command:</strong> Runs a Bash script that:</li>
                            <ul>
                                <li>Verifies required environment variables like <strong><i>ES_PASSWORD</i></strong> and <strong><i>KIBANA_PASSWORD.</i></strong></li>
                                <li>Generates a certificate authority (CA) and individual SSL certificates for Elasticsearch, Kibana, and other services.</li>
                                <li>Sets the Kibana system password.</li>
                            </ul>
                            <li><strong>Volumes:</strong> Uses certs volume to store generated certificates.</li>
                            <li><strong>Networks:</strong> Connected to <strong><i>elastic_network.</i></strong></li>
                            <li><strong>Healthcheck:</strong> Verifies that the required certificates exist before marking the service as healthy.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">services:
    setup:
        image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
        networks:
            - elastic_network
        volumes:
            - certs:/usr/share/elasticsearch/config/certs
        user: "0"
        command: >
            bash -c '
              if [ x${ES_PASSWORD} == x ]; then
                echo "Set the ES_PASSWORD environment variable in the .env file";
                exit 1;
              elif [ x${KIBANA_PASSWORD} == x ]; then
                echo "Set the KIBANA_PASSWORD environment variable in the .env file";
                exit 1;
              fi;
              if [ ! -f config/certs/ca.zip ]; then
                echo "Creating CA";
                bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;
                unzip config/certs/ca.zip -d config/certs;
              fi;
              if [ ! -f config/certs/certs.zip ]; then
                echo "Creating certs";
                echo -ne \
                "instances:\n"\
                "  - name: es-coord\n"\
                "    dns:\n"\
                "      - es-coord\n"\
                "      - localhost\n"\
                "    ip:\n"\
                "      - 127.0.0.1\n"\
                "  - name: es-masternode-01\n"\
                "    dns:\n"\
                "      - es-masternode-01\n"\
                "      - localhost\n"\
                "    ip:\n"\
                "      - 127.0.0.1\n"\
                "  - name: es-masternode-02\n"\
                "    dns:\n"\
                "      - es-masternode-02\n"\
                "      - localhost\n"\
                "    ip:\n"\
                "      - 127.0.0.1\n"\
                "  - name: es-masternode-03\n"\
                "    dns:\n"\
                "      - es-masternode-03\n"\
                "      - localhost\n"\
                "    ip:\n"\
                "      - 127.0.0.1\n"\
                "  - name: kibana-01\n"\
                "    dns:\n"\
                "      - kibana-01\n"\
                "      - localhost\n"\
                "    ip:\n"\
                "      - 127.0.0.1\n"\
                "  - name: kibana-02\n"\
                "    dns:\n"\
                "      - kibana-02\n"\
                "      - localhost\n"\
                "    ip:\n"\
                "      - 127.0.0.1\n"\
                "  - name: logstash\n"\
                "    dns:\n"\
                "      - logstash\n"\
                "      - localhost\n"\
                "    ip:\n"\
                "      - 127.0.0.1\n"\
                "  - name: nginx\n"\
                "    dns:\n"\
                "      - nginx\n"\
                "      - nginx\n"\
                "    ip:\n"\
                "      - 127.0.0.1\n"\
                > config/certs/instances.yml;
                bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;
                unzip config/certs/certs.zip -d config/certs;
              fi;
              # Ensure Kibana certificates are moved to the correct directory
              if [ ! -d config/certs/kibana ]; then
                mkdir -p config/certs/kibana;
              fi;
              mv config/certs/kibana-01/* config/certs/kibana/;
              mv config/certs/kibana-02/* config/certs/kibana/;
              echo "Setting file permissions"
              chown -R root:root config/certs;
              find . -type d -exec chmod 750 \{\} \;;
              find . -type f -exec chmod 640 \{\} \;;
              echo "Waiting for Elasticsearch availability";
              until curl -s --cacert config/certs/ca/ca.crt https://es-coord:9200 | grep -q "missing authentication credentials"; do sleep 30; done;
              echo "Setting kibana_system password";
              until curl -s -X POST --cacert config/certs/ca/ca.crt -u "${ES_USER}:${ES_PASSWORD}" -H "Content-Type: application/json" https://es-coord:9200/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 10; done;
              echo "All done!";
            '
        healthcheck:
            test: ["CMD-SHELL", "[ -f config/certs/es-coord/es-coord.crt ]"]
            interval: 1s
            timeout: 5s
            retries: 120
</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Elasticsearch Coordination Node (es-coord)</strong></h6>
                        <ul>
                            <li><strong>Purpose:</strong> Routes client requests from Kibana and Logstash to the appropriate Elasticsearch master nodes. Does not store data itself.</li>
                            <li><strong>Image:</strong> Elasticsearch image defined by <strong><i>${STACK_VERSION}</i></strong>.</li>
                            <li><strong>Hostname:</strong> <strong><i>sheild-es-coord</i></strong> — Unique hostname for internal networking.</li>
                            <li><strong>Ports:</strong> Exposes Elasticsearch on <strong><i>${ES_PORT}</i></strong> (typically 9200) for API interactions.</li>
                            <li><strong>Volumes:</strong> Stores SSL certificates in certs and uses <strong><i>esdata-coord</i></strong> for data.</li>
                            <li><strong>Environment Variables:</strong></li>
                            <ul>
                                <li><strong>node.name:</strong> Identifies the node.</li>
                                <li><strong>cluster.name:</strong> Defines the cluster name.</li>
                                <li><strong>discovery.seed_hosts:</strong> Specifies other master nodes for cluster discovery.</li>
                                <li><strong>xpack.security:</strong> Enables SSL and sets up certificates for secure communication.</li>
                            </ul>
                            <li><strong>Mem_limit:</strong> Restricts memory usage based on <strong><i>${ES_MEM_LIMIT}</i></strong> (to prevent excessive resource usage).</li>
                            <li><strong>Healthcheck:</strong> Verifies that the service is available by testing the certificate-based secure connection.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">    es-coord:
        image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
        hostname: sheild-es-coord
        container_name: marvel-es-coord
        depends_on:
            setup:
                condition: service_healthy
        volumes:
            - certs:/usr/share/elasticsearch/config/certs
            - esdata-coord:/usr/share/elasticsearch/data
        ports:
            - ${ES_PORT}:${ES_PORT}
        networks:
            - elastic_network
        environment:
            - node.name=es-coord
            - node.roles=!!seq ""
            - cluster.name=${CLUSTER_NAME}
            - cluster.initial_master_nodes=es-masternode-01,es-masternode-02,es-masternode-03
            - discovery.seed_hosts=es-masternode-01,es-masternode-02,es-masternode-03
            - ELASTIC_PASSWORD=${ES_PASSWORD}
            - bootstrap.memory_lock=true
            - xpack.security.enabled=true
            - xpack.security.http.ssl.enabled=true
            - xpack.security.http.ssl.key=certs/es-coord/es-coord.key
            - xpack.security.http.ssl.certificate=certs/es-coord/es-coord.crt
            - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
            - xpack.security.transport.ssl.enabled=true
            - xpack.security.transport.ssl.key=certs/es-coord/es-coord.key
            - xpack.security.transport.ssl.certificate=certs/es-coord/es-coord.crt
            - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
            - xpack.security.transport.ssl.verification_mode=certificate
            - xpack.license.self_generated.type=${LICENSE}
            - ES_JAVA_OPTS=-Xms512m -Xmx512m
        mem_limit: ${ES_MEM_LIMIT}
        ulimits:
            memlock:
                soft: -1
                hard: -1
        healthcheck:
            test:
                [
                   "CMD-SHELL",
                   "curl -s --cacert config/certs/ca/ca.crt https://es-coord:${ES_PORT} | grep -q 'missing authentication credentials'"
                ]
            interval: 10s
            timeout: 10s
            retries: 120
</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Elasticsearch Master Nodes (es-masternode-01, es-masternode-02, es-masternode-03)</strong></h6>
                        <ul>
                            <li><strong>Purpose:</strong> Store and manage the Elasticsearch data across the cluster. Each node is responsible for indexing and searching.</li>
                            <li><strong>Image:</strong> Uses the same Elasticsearch image as the coordination node.</li>
                            <li><strong>Ports:</strong> Exposes each node on different ports <strong><i>(9201, 9202, 9203)</i></strong>.</li>
                            <li><strong>Volumes:</strong> Each node has a separate data volume <strong><i>(es-data-01, es-data-02, es-data-03)</i></strong> and shares the certs volume for SSL certificates.</li>
                            <li><strong>Environment Variables:</strong></li>
                            <ul>
                                <li>Similar to the coordination node, but with <strong><i>node.roles=master,data</i></strong> indicating this node handles both master and data roles.</li>
                                <li>Sets memory usage and configures SSL.</li>
                            </ul>
                            <li><strong>Mem_limit:</strong> Restricts memory usage based on <strong><i>${ES_MEM_LIMIT}</i></strong> (to prevent excessive resource usage).</li>
                            <li><strong>Healthcheck:</strong> Verifies that the node is healthy by checking for a successful SSL connection on its respective port.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">    es-masternode-01:
        image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
        hostname: sheild-es-masternode-01
        container_name: marvel-es-masternode-01
        depends_on:
            es-coord:
                condition: service_healthy
        volumes:
            - certs:/usr/share/elasticsearch/config/certs
            - es-data-01:/usr/share/elasticsearch/data
        networks:
            - elastic_network
        ports:
            - 9201:${ES_PORT}
        environment:
            - node.name=es-masternode-01
            - node.roles=master,data
            - cluster.name=${CLUSTER_NAME}
            - cluster.initial_master_nodes=es-masternode-01,es-masternode-02,es-masternode-03
            - discovery.seed_hosts=es-masternode-01,es-masternode-02,es-masternode-03
            - ELASTIC_PASSWORD=${ES_PASSWORD}
            - bootstrap.memory_lock=true
            - xpack.security.enabled=true
            - xpack.security.http.ssl.enabled=true
            - xpack.security.http.ssl.key=certs/es-masternode-01/es-masternode-01.key
            - xpack.security.http.ssl.certificate=certs/es-masternode-01/es-masternode-01.crt
            - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
            - xpack.security.transport.ssl.enabled=true
            - xpack.security.transport.ssl.key=certs/es-masternode-01/es-masternode-01.key
            - xpack.security.transport.ssl.certificate=certs/es-masternode-01/es-masternode-01.crt
            - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
            - xpack.security.transport.ssl.verification_mode=certificate
            - xpack.license.self_generated.type=${LICENSE}
            - ES_JAVA_OPTS=-Xms512m -Xmx512m
        mem_limit: ${ES_MEM_LIMIT}
        ulimits:
            memlock:
                soft: -1
                hard: -1
        healthcheck:
            test:
                [
                   "CMD-SHELL",
                   "curl -s --cacert config/certs/ca/ca.crt https://sheild-es-masternode-01:${ES_PORT} | grep -q 'missing authentication credentials'"
                ]
            interval: 10s
            timeout: 10s
            retries: 120

    es-masternode-02:
        image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
        hostname: sheild-es-masternode-02
        container_name: marvel-es-masternode-02
        depends_on:
            es-coord:
                condition: service_healthy
        volumes:
            - certs:/usr/share/elasticsearch/config/certs
            - es-data-02:/usr/share/elasticsearch/data
        networks:
            - elastic_network
        ports:
            - 9202:${ES_PORT}
        environment:
            - node.name=es-masternode-02
            - node.roles=master,data
            - cluster.name=${CLUSTER_NAME}
            - cluster.initial_master_nodes=es-masternode-01,es-masternode-02,es-masternode-03
            - discovery.seed_hosts=es-masternode-01,es-masternode-02,es-masternode-03
            - ELASTIC_PASSWORD=${ES_PASSWORD}
            - bootstrap.memory_lock=true
            - xpack.security.enabled=true
            - xpack.security.http.ssl.enabled=true
            - xpack.security.http.ssl.key=certs/es-masternode-02/es-masternode-02.key
            - xpack.security.http.ssl.certificate=certs/es-masternode-02/es-masternode-02.crt
            - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
            - xpack.security.transport.ssl.enabled=true
            - xpack.security.transport.ssl.key=certs/es-masternode-02/es-masternode-02.key
            - xpack.security.transport.ssl.certificate=certs/es-masternode-02/es-masternode-02.crt
            - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
            - xpack.security.transport.ssl.verification_mode=certificate
            - xpack.license.self_generated.type=${LICENSE}
            - ES_JAVA_OPTS=-Xms512m -Xmx512m
        mem_limit: ${ES_MEM_LIMIT}
        ulimits:
            memlock:
                soft: -1
                hard: -1
        healthcheck:
            test:
                [
                   "CMD-SHELL",
                   "curl -s --cacert config/certs/ca/ca.crt https://sheild-es-masternode-02:${ES_PORT} | grep -q 'missing authentication credentials'"
                ]
            interval: 10s
            timeout: 10s
            retries: 120

    es-masternode-03:
        image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
        hostname: sheild-es-masternode-03
        container_name: marvel-es-masternode-03
        depends_on:
            es-coord:
                condition: service_healthy
        volumes:
            - certs:/usr/share/elasticsearch/config/certs
            - es-data-03:/usr/share/elasticsearch/data
        networks:
            - elastic_network
        ports:
            - 9203:${ES_PORT}
        environment:
            - node.name=es-masternode-03
            - node.roles=master,data
            - cluster.name=${CLUSTER_NAME}
            - cluster.initial_master_nodes=es-masternode-01,es-masternode-02,es-masternode-03
            - discovery.seed_hosts=es-masternode-01,es-masternode-02,es-masternode-03
            - ELASTIC_PASSWORD=${ES_PASSWORD}
            - bootstrap.memory_lock=true
            - xpack.security.enabled=true
            - xpack.security.http.ssl.enabled=true
            - xpack.security.http.ssl.key=certs/es-masternode-03/es-masternode-03.key
            - xpack.security.http.ssl.certificate=certs/es-masternode-03/es-masternode-03.crt
            - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
            - xpack.security.transport.ssl.enabled=true
            - xpack.security.transport.ssl.key=certs/es-masternode-03/es-masternode-03.key
            - xpack.security.transport.ssl.certificate=certs/es-masternode-03/es-masternode-03.crt
            - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
            - xpack.security.transport.ssl.verification_mode=certificate
            - xpack.license.self_generated.type=${LICENSE}
            - ES_JAVA_OPTS=-Xms512m -Xmx512m
        mem_limit: ${ES_MEM_LIMIT}
        ulimits:
            memlock:
                soft: -1
                hard: -1
        healthcheck:
            test:
                [
                   "CMD-SHELL",
                   "curl -s --cacert config/certs/ca/ca.crt https://sheild-es-masternode-03:${ES_PORT} | grep -q 'missing authentication credentials'"
                ]
            interval: 10s
            timeout: 10s
            retries: 120
</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Kibana Instances (kibana-01, kibana-02)</strong></h6>
                        <ul>
                            <li><strong>Purpose:</strong> Provides a graphical interface for querying and visualizing data in Elasticsearch. There are two instances of Kibana for load balancing.</li>
                            <li><strong>Image:</strong> Official Kibana image from Elastic.</li>
                            <li><strong>Volumes:</strong> Uses the certs volume for certificates and <strong><i>kibana-data-01/kibana-data-02</i></strong> for persistent Kibana data.</li>
                            <li><strong>Ports:</strong> Exposes Kibana on <strong><i>${KIBANA_PORT}</i></strong> (e.g., <i>5601</i> for <i>kibana-01</i> and <i>5602</i> for <i>kibana-02</i>).</li>
                            <li><strong>Environment Variables:</strong></li>
                            <ul>
                                <li><strong>SERVER_SSL_ENABLED:</strong> Enables SSL for secure communication.</li>
                                <li>Sets memory usage and configures SSL.</li>
                                <li><strong>ELASTICSEARCH_USERNAME</strong> and <strong>ELASTICSEARCH_PASSWORD:</strong> Used for secure access to Elasticsearch.</li>
                            </ul>
                            <li><strong>Healthcheck:</strong> Ensures Kibana is running and accessible via HTTPS.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">    kibana-01:
        image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
        hostname: sheild-kibana-01
        container_name: marvel-kibana-01
        depends_on:
            es-coord:
                condition: service_healthy
        volumes:
            - certs:/usr/share/kibana/config/certs
            - kibana-data-01:/usr/share/kibana/data
        networks:
            - elastic_network
        ports:
            - ${KIBANA_PORT}:${KIBANA_PORT}
        environment:
            - SERVER_NAME=kibana
            - ELASTICSEARCH_HOSTS=https://es-coord:${ES_PORT}
            - ELASTICSEARCH_USERNAME=${KIBANA_USER}
            - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
            - SERVER_SSL_ENABLED=true
            - SERVER_SSL_CERTIFICATE=config/certs/kibana/kibana-01.crt
            - SERVER_SSL_KEY=config/certs/kibana/kibana-01.key
            - SERVER_SSL_CERTIFICATE_AUTHORITIES=config/certs/ca/ca.crt
            - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
            - SERVER_PUBLICBASEURL=https://kibana-01:${KIBANA_PORT}
        mem_limit: ${KB_MEM_LIMIT}
        healthcheck:
            test:
                [
                   "CMD-SHELL",
                   "curl -s --cacert /usr/share/kibana/config/certs/kibana/kibana-01.crt --fail --insecure -o /dev/null https://sheild-kibana-01:${KIBANA_PORT} || exit 1"
                ]
            interval: 10s
            timeout: 10s
            retries: 300

    kibana-02:
        image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
        hostname: sheild-kibana-02
        container_name: marvel-kibana-02
        depends_on:
            es-coord:
                condition: service_healthy
        volumes:
            - certs:/usr/share/kibana/config/certs
            - kibana-data-02:/usr/share/kibana/data
        networks:
            - elastic_network
        ports:
            - 5602:${KIBANA_PORT}
        environment:
            - SERVER_NAME=kibana
            - ELASTICSEARCH_HOSTS=https://es-coord:${ES_PORT}
            - ELASTICSEARCH_USERNAME=${KIBANA_USER}
            - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
            - SERVER_SSL_ENABLED=true
            - SERVER_SSL_CERTIFICATE=config/certs/kibana/kibana-02.crt
            - SERVER_SSL_KEY=config/certs/kibana/kibana-02.key
            - SERVER_SSL_CERTIFICATE_AUTHORITIES=config/certs/ca/ca.crt
            - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
            - SERVER_PUBLICBASEURL=https://kibana-02:${KIBANA_PORT}
        mem_limit: ${KB_MEM_LIMIT}
        healthcheck:
            test:
                [
                   "CMD-SHELL",
                   "curl -s --cacert /usr/share/kibana/config/certs/kibana/kibana-01.crt --fail --insecure -o /dev/null https://sheild-kibana-02:${KIBANA_PORT} || exit 1"
                ]
            interval: 10s
            timeout: 10s
            retries: 300
</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Logstash</strong></h6>
                        <ul>
                            <li><strong>Purpose:</strong> Ingests data from Kafka and pushes it to Elasticsearch for indexing.</li>
                            <li><strong>Build:</strong> Custom build from the logstash directory with the provided <strong><i>Dockerfile</i></strong>.</li>
                            <li><strong>Networks:</strong> Connected to both <strong><i>elastic_network</i></strong> and <strong><i>kafka_network</i></strong> to ingest data from Kafka and forward it to Elasticsearch.</li>
                            <li><strong>Ports:</strong> Exposes port <strong><i>5044</i></strong> for filebeat or other input connections.</li>
                            <li><strong>Environment Variables:</strong></li>
                            <ul>
                                <li><strong>LS_JAVA_OPTS:</strong> Configures memory settings for Logstash.</li>
                                <li><strong>xpack.monitoring.enabled:</strong> Disables monitoring to reduce overhead.</li>
                                <li><strong>Command:</strong> Runs Logstash with a custom configuration file <strong><i>(logstash.conf)</i></strong> for Kafka-to-Elasticsearch processing.</li>
                            </ul>
                            <li><strong>Healthcheck:</strong> Verifies Logstash's connection to Elasticsearch.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">    logstash:
        build:
            context: ./logstash
            dockerfile: Dockerfile
        hostname: sheild-logstash
        container_name: marvel-logstash
        depends_on:
            es-coord:
                condition: service_healthy
            kafka:
                condition: service_healthy
        labels:
            co.elastic.logs/module: logstash
        user: root
        volumes:
            - certs:/usr/share/logstash/config/certs
        networks:
            - elastic_network
            - kafka_network
        ports:
            - 5044:5044
        environment:
            - NODE_NAME="logstash"
            - xpack.monitoring.enabled=false
            - LS_JAVA_OPTS=-Xmx1g -Xms1g
        command: logstash -f /usr/share/logstash/pipeline/logstash.conf
        healthcheck:
            test: ["CMD-SHELL", "curl --cacert /usr/share/logstash/config/certs/ca.crt https://logstash:${ES_PORT} -u ${ES_USER}:${ES_PASSWORD} -o /dev/null -w \"%{http_code}\" -s | grep -q 200"]
            interval: 10s
            timeout: 10s
            retries: 3
</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Zookeeper</strong></h6>
                        <ul>
                            <li><strong>Purpose:</strong> Coordinates the Kafka brokers by managing the distributed configuration and ensuring fault-tolerance.</li>
                            <li><strong>Image:</strong> Confluent's Zookeeper image.</li>
                            <li><strong>Ports:</strong> Exposes Zookeeper on the default port <strong><i>(2181)</i></strong>.</li>
                            <li><strong>Environment Variables:</strong></li>
                            <ul>
                                <li><strong>ZOOKEEPER_CLIENT_PORT:</strong> Specifies the Zookeeper client port.</li>
                                <li><strong>ZOOKEEPER_TICK_TIME:</strong> Defines the basic time unit for Zookeeper's heartbeat mechanism.</li>
                            </ul>
                            <li><strong>Healthcheck:</strong> Pings the Zookeeper server to ensure it is running.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">    zookeeper:
        image: confluentinc/cp-zookeeper:latest
        hostname: sheild-zookeeper
        container_name: marvel-zookeeper
        environment:
            - ZOOKEEPER_CLIENT_PORT=${ZK_PORT}
            - ZOOKEEPER_TICK_TIME=2000
        ports:
            - ${ZK_PORT}:${ZK_PORT}
        networks:
            - kafka_network
        healthcheck:
          test: [ "CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181" ]
          interval: 10s
          retries: 3
          start_period: 30s
          timeout: 5s
</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Kafka</strong></h6>
                        <ul>
                            <li><strong>Purpose:</strong> Acts as the message broker, receiving data from the Python banking app and allowing Logstash to consume the data.</li>
                            <li><strong>Image:</strong> Confluent Kafka image.</li>
                            <li><strong>Ports:</strong> Exposes Kafka on <strong><i>${KAFKA_PORT}</i></strong> for communication.</li>
                            <li><strong>Environment Variables:</strong></li>
                            <ul>
                                <li><strong>KAFKA_ZOOKEEPER_CONNECT:</strong> Connects Kafka to Zookeeper.</li>
                                <li><strong>KAFKA_ADVERTISED_LISTENERS:</strong> Defines how Kafka is accessible from within and outside the Docker network.</li>
                                <li><strong>Command:</strong> Creates the Kafka topic <strong><i>(${KAFKA_TOPIC_1})</i></strong> before starting Kafka.</li>
                            </ul>
                            <li><strong>Healthcheck:</strong> Verifies Kafka is running and accessible.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">    kafka:
        hostname: sheild-kafka
        container_name: marvel-kafka
        depends_on:
            zookeeper:
                condition: service_healthy
        image: confluentinc/cp-kafka:latest
        ports:
            - ${KAFKA_PORT}:${KAFKA_PORT}
        environment:
            - KAFKA_BROKER_ID=1
            - KAFKA_ZOOKEEPER_CONNECT=zookeeper:${ZK_PORT}
            - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${KAFKA_BOOTSTRAP_SERVERS},PLAINTEXT_HOST://localhost:29092
            - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
            - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
            - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
        command: sh -c "((sleep 120 && kafka-topics --create --topic ${KAFKA_TOPIC_1} --bootstrap-server ${KAFKA_BOOTSTRAP_SERVERS} --partitions 1 --replication-factor 1)&) && /etc/confluent/docker/run ">
        networks:
            - kafka_network
        healthcheck:
            test: ["CMD-SHELL", "nc -z kafka ${KAFKA_PORT} || exit 1"]
            interval: 30s
            retries: 3
            start_period: 60s
            timeout: 10s
</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Python Banking Application (banking-app)</strong></h6>
                        <ul>
                            <li><strong>Purpose:</strong> Simulates the data producer, generating synthetic banking transactions and sending them to Kafka.</li>
                            <li><strong>Build:</strong> Built from the banking directory with the provided <i>Dockerfile</i>.</li>
                            <li><strong>Networks:</strong> Connected to the <strong><i>kafka_network</i></strong> for communication with Kafka.</li>
                            <li><strong>Environment Variables:</strong></li>
                            <ul>
                                <li><strong>KAFKA_BOOTSTRAP_SERVERS:</strong> Defines the Kafka bootstrap server address.</li>
                                <li><strong>KAFKA_TOPIC:</strong> Defines the Kafka topic where the banking transactions are published.</li>
                                <li><strong>Command:</strong> Runs the Python script <strong><i>(banking_app.py)</i></strong> to generate and publish data.</li>
                            </ul>
                            <li><strong>Healthcheck:</strong> Verifies Kafka connectivity to ensure proper data production.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">    banking-app:
        hostname: sheild-banking-app
        container_name: marvel-banking-app
        depends_on:
            kafka:
                condition: service_healthy
        build:
            context: ./banking
            dockerfile: Dockerfile
        networks:
            - kafka_network
        working_dir: /usr/src/app
        environment:
            - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
            - KAFKA_TOPIC=${KAFKA_TOPIC_1}
        command: ["python3", "/usr/src/app/banking_app.py"]
        healthcheck:
            test: ["CMD-SHELL", "nc -z kafka ${KAFKA_PORT} || exit 1"]
            interval: 30s
            timeout: 10s
            retries: 3
</code></pre>
                        </div>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">9.2 .env [/project-diroctory]</h3>
                    <div class="content">
                        <p>The <strong><i>.env</i></strong> file is critical for configuring environment variables that are used throughout the <strong><i>docker-compose.yml</i></strong> file and Docker containers. It provides centralized settings that allow for dynamic configuration of the services without hardcoding values in the <strong><i>docker-compose.yml</i></strong> file. Here is a detailed breakdown of each environment variable used:</p>
                        <ul>
                            <li><strong>STACK_VERSION:</strong> Specifies the version of the Elastic Stack products (Elasticsearch, Logstash, Kibana). Here, version 8.15.0 is used, which ensures all Elastic components run on the same compatible version.</li>
                            <li><strong>COMPOSE_PROJECT_NAME:</strong> Defines the project name for Docker Compose. In this case, techsavvyrc is used, which acts as a namespace for all containers and resources (e.g., network, volume names). It helps avoid conflicts with other projects running on the same Docker host.</li>
                            <li><strong>CLUSTER_NAME:</strong> Defines the name of the Elasticsearch cluster. Here, <strong><i>marvel</i></strong> is used, and this name helps to identify the cluster in a multi-cluster setup. All nodes belonging to this cluster will use this name for coordination.</li>
                            <li><strong>LICENSE:</strong> Specifies the type of Elastic Stack license. Setting it to basic enables the free tier of Elasticsearch and Kibana, which includes essential features. You could also set this to trial to enable the 30-day trial of enterprise features.</li>
                            <li><strong>ES_USER</strong> and <strong>ES_PASSWORD:</strong> Credentials for accessing Elasticsearch. These values (elastic and elastic) define the default user and password required for connecting to Elasticsearch and are used for both internal communication (Logstash, Kibana) and external API access.</li>
                            <li><strong>ES_PORT:</strong> Defines the port number on which Elasticsearch is exposed. The default port for Elasticsearch is <strong><i>9200</i></strong>, and this is used by Kibana and Logstash to communicate with Elasticsearch.</li>
                            <li><strong>ES_INDEX_1</strong> and <strong>ES_INDEX_2:</strong> These variables define the indices (data storage units) for Elasticsearch. In this setup, <strong><i>bank_transactions</i></strong> and <strong><i>ecom_transactions</i></strong> are used as two separate indices for storing banking and e-commerce transaction data, respectively.</li>
                            <li><strong>KIBANA_USER</strong> and <strong>KIBANA_PASSWORD:</strong> These are the credentials for the <strong><i>kibana_system</i></strong> user, which Kibana uses to authenticate with Elasticsearch. The same password (elastic) is shared between Elasticsearch and Kibana for simplicity.</li>
                            <li><strong>KIBANA_PORT:</strong> Defines the port number on which Kibana will be exposed. Here, it is set to 5601, which is the default port for accessing Kibana’s web interface.</li>
                            <li><strong>ES_MEM_LIMIT:</strong> Defines the memory limit for Elasticsearch containers. The value 4294967296 bytes translates to 4 GB of memory, which restricts the maximum memory that Elasticsearch can use, ensuring it does not exceed the available host memory.</li>
                            <li><strong>KB_MEM_LIMIT:</strong> Defines the memory limit for Kibana containers. Set to 1073741824 bytes, which equals 1 GB of memory for Kibana.</li>
                            <li><strong>LS_MEM_LIMIT:</strong> Defines the memory limit for Logstash containers. Like Kibana, it is also set to 1 GB (1073741824 bytes), which is sufficient for moderate ingestion workloads.</li>
                            <li><strong>ZK_PORT:</strong> Defines the port for Zookeeper, which is part of the Kafka infrastructure. The port <strong><i>2181</i></strong> is the default client port for Zookeeper, used for managing and coordinating Kafka brokers.</li>
                            <li><strong>KAFKA_BOOTSTRAP_SERVERS:</strong> Specifies the Kafka bootstrap server, which is the entry point for clients to connect to the Kafka cluster. In this setup, <strong><i>kafka:9092</i></strong> means Kafka is accessible on the internal hostname kafka via port <strong><i>9092</i></strong>.</li>
                            <li><strong>KAFKA_PORT:</strong> Defines the port number on which Kafka is exposed. Port 9092 is the standard port for Kafka communication and is used by Logstash and the Python banking app to send and receive messages.</li>
                            <li><strong>KAFKA_TOPIC_1</strong> and <strong>KAFKA_TOPIC_2:</strong> These variables define the Kafka topics used to categorize messages. In this setup, <strong><i>bank_transactions</i></strong> and <strong><i>ecom_transactions</i></strong> are the two Kafka topics where the banking and e-commerce transaction data is published, respectively.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml"># Version of Elastic products
STACK_VERSION=8.15.0

# Project namespace (defaults to the current folder name if not set)
COMPOSE_PROJECT_NAME=techsavvyrc

# Set the cluster name
CLUSTER_NAME=marvel

# Set to 'basic' or 'trial' to automatically start the 30-day trial
LICENSE=basic

# Elastic settings
ES_USER=elastic
ES_PASSWORD=elastic
ES_PORT=9200
ES_INDEX_1=bank_transactions
ES_INDEX_2=ecom_transactions

# Kibana settings
KIBANA_USER=kibana_system
KIBANA_PASSWORD=elastic
KIBANA_PORT=5601

# Increase or decrease based on the available host memory (in bytes)
ES_MEM_LIMIT=4294967296
KB_MEM_LIMIT=1073741824
LS_MEM_LIMIT=1073741824

# Zookeeper Settings
ZK_PORT=2181

# Kafka Settings
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_PORT=9092
KAFKA_TOPIC_1=bank_transactions
KAFKA_TOPIC_2=ecom_transactions
</code></pre>
                        </div>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">9.3 .env [/project-directory/logstash]</h3>
                    <div class="content">
                        <p>This <strong><i>.env</i></strong> file under the logstash directory provides essential configuration details that Logstash uses to interact with both Elasticsearch and Kafka. These variables simplify the configuration of Logstash by centralizing key values that can be referenced within the Logstash configuration files (like <strong><i>logstash.conf</i></strong>). Here's a detailed breakdown of each environment variable:</p>
                        <ul>
                            <li><strong>ES_USER:</strong> The username <strong><i>(elastic)</i></strong> for authenticating with Elasticsearch. This is used by Logstash to securely connect to Elasticsearch and push data into the defined indices.</li>
                            <li><strong>ES_PASSWORD:</strong> The password <strong><i>(elastic)</i></strong> for the Elasticsearch user. This ensures that Logstash can authenticate successfully with Elasticsearch when indexing data. This credential matches the one defined in the main <strong><i>.env</i></strong> file to ensure consistency across services.</li>
                            <li><strong>ES_PORT:</strong> Specifies the port number <strong><i>(9200)</i></strong> on which Elasticsearch is running. Logstash needs this to know where to send data for indexing. Port <strong><i>9200</i></strong> is the standard port for Elasticsearch’s REST API, used for both searching and indexing data.</li>
                            <li><strong>ES_INDEX_1</strong> and <strong>ES_INDEX_2:</strong> These variables define the Elasticsearch indices where Logstash will store ingested data.</li>
                            <ul>
                                <li><strong>ES_INDEX_1 (bank_transactions):</strong> Used to store data related to banking transactions.</li>
                            </ul>
                            <li><strong>KAFKA_BOOTSTRAP_SERVERS:</strong> This defines the Kafka bootstrap server address <strong><i>(kafka:9092)</i></strong>, which Logstash connects to for consuming messages. The kafka hostname refers to the internal service name in the Docker network, and port <strong><i>9092</i></strong> is Kafka's default communication port.</li>
                            <li><strong>KAFKA_PORT:</strong> Specifies the port number <strong><i>(9200)</i></strong> for Kafka communication. This is the port on which Kafka listens for incoming connections from clients, such as Logstash, which subscribes to Kafka topics to ingest data.</li>
                            <li><strong>KAFKA_TOPIC_1 and KAFKA_TOPIC_2:</strong> These define the Kafka topics that Logstash will consume from.</li>
                            <ul>
                                <li><strong>KAFKA_TOPIC_1 (bank_transactions):</strong> Logstash listens to this topic for banking transaction data generated by the Python banking app.</li>
                            </ul>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">ES_USER=elastic
ES_PASSWORD=elastic
ES_PORT=9200
ES_INDEX_1=bank_transactions
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_PORT=9092
KAFKA_TOPIC_1=bank_transactions
</code></pre>
                        </div>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">9.4 Dockerfile [/project-directory/logstash]</h3>
                    <div class="content">
                        <p>This <strong><i>Dockerfile</i></strong> defines the custom image and environment for running the Logstash service in your Docker-based ELK stack setup. The image builds on the official Logstash base image and customizes it by installing additional packages, copying configuration files, and setting up an entry point script. Here’s a detailed explanation of each instruction:</p>
                        <h6 style="color: #149ddd;"><strong>Base Image</strong></h6>
                        <p>This line pulls the official Logstash image, version 8.15.0, from the Elastic repository. This base image contains the core Logstash functionality and is used as the starting point for building a custom Logstash container.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">FROM docker.elastic.co/logstash/logstash:8.15.0</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Working Directory</strong></h6>
                        <p>Sets the working directory within the container to <strong><i>/usr/share/logstash/pipeline</i></strong>. This is where all subsequent commands (like copying files) will be executed and is the directory where Logstash expects its configuration files (pipelines) to be located.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">WORKDIR /usr/share/logstash/pipeline</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>User Privileges</strong></h6>
                        <p>Switches to the root user to install additional system packages. The default user for the Logstash container does not have the necessary privileges to install packages, so this is required to perform system-level changes.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">USER root</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Package Installation</strong></h6>
                        <p>Installs several packages using <strong><i>apt-get</i></strong>. These packages provide additional tools needed for debugging, networking, and environment variable substitution:</p>
                        <ul>
                            <li><strong>gettext-base:</strong>. Provides the envsubst tool, used for substituting environment variables into configuration files.</li>
                            <li><strong>vim:</strong>. A text editor, useful for debugging inside the container.</li>
                            <li><strong>net-tools, inetutils-ping:</strong>. Networking tools used to check network connections and interfaces.</li>
                            <li><strong>netcat-traditional:</strong>. Used for testing network connections, often helpful for verifying Kafka connections and other network-based services.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">RUN apt-get update && apt-get install -y gettext-base vim net-tools inetutils-ping netcat-traditional</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Copy Environment and Configuration Files</strong></h6>
                        <p>These commands copy important files from the local directory (where the <strong><i>Dockerfile</i></strong> is located) into the container’s working directory (/usr/share/logstash/pipeline):</p>
                        <ul>
                            <li><strong>.env:</strong> The environment variables file. These variables are referenced in the Logstash configuration file and are used to dynamically configure Logstash.</li>
                            <li><strong>logstash.template.conf:</strong> The Logstash pipeline configuration template, which contains placeholders for environment variables (e.g., Kafka topic names, Elasticsearch credentials). This file defines how data flows from Kafka to Elasticsearch.</li>
                            <li><strong>entrypoint.sh:</strong> A script that will be executed when the container starts. It dynamically substitutes environment variables into the <strong><i>logstash.template.conf</i></strong> to create a final <strong><i>logstash.conf</i></strong> file that Logstash will use.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">COPY .env /usr/share/logstash/pipeline
COPY logstash.template.conf /usr/share/logstash/pipeline
COPY entrypoint.sh /usr/share/logstash/pipeline
</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Set File Permissions</strong></h6>
                        <p>Grants execution permissions (+x) to the entrypoint.sh script, allowing it to be run when the container starts.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">RUN chmod +x /usr/share/logstash/pipeline/entrypoint.sh</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Entry Point</strong></h6>
                        <p>Sets the entrypoint for the container. This means that when the container is run, it will execute the <strong><i>entrypoint.sh</i></strong> script. The script is responsible for:</p>
                        <ul>
                            <li>Substituting environment variables in the <strong><i>logstash.template.conf</i></strong>.</li>
                            <li>Creating the final <strong><i>logstash.conf</i></strong> configuration file.</li>
                            <li>Starting Logstash with the generated configuration.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">ENTRYPOINT ["/usr/share/logstash/pipeline/entrypoint.sh"]</code></pre>
                        </div>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml"># Use Logstash base image
FROM docker.elastic.co/logstash/logstash:8.15.0

# Set working directory
WORKDIR /usr/share/logstash/pipeline

# Switch to root user to install packages
USER root

# Install requied packages
RUN apt-get update && apt-get install -y gettext-base vim net-tools inetutils-ping netcat-traditional

# Copy scripts and configuration templates
COPY .env /usr/share/logstash/pipeline
COPY logstash.template.conf /usr/share/logstash/pipeline
COPY entrypoint.sh /usr/share/logstash/pipeline

# Give execution rights to the script
RUN chmod +x /usr/share/logstash/pipeline/entrypoint.sh

# Substitute environment variables in the Python script
ENTRYPOINT ["/usr/share/logstash/pipeline/entrypoint.sh"]
</code></pre>
                        </div>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">9.5 entrypoint.sh [/project-directory/logstash]</h3>
                    <div class="content">
                        <p>The <strong><i>entrypoint.sh</i></strong> script is responsible for dynamically configuring Logstash at container startup. It reads environment variables from the <strong><i>.env</i></strong> file, substitutes these values into the Logstash configuration template, and then launches Logstash with the finalized configuration. This script is crucial for ensuring that Logstash operates correctly with dynamically set values (e.g., Kafka topics, Elasticsearch credentials).</p>
                        <p>The <strong><i>entrypoint.sh</i></strong> script performs the following key tasks:</p>
                        <ol>
                            <li>Exports environment variables from the <strong><i>.env</i></strong> file, making them available for use in the script.</li>
                            <li>Substitutes the environment variables into the Logstash configuration template <strong><i>(logstash.template.conf)</i></strong>, generating the final <strong><i>logstash.conf</i></strong> file.</li>
                            <li>Ensures correct permissions on the generated configuration file so that Logstash can access it.</li>
                            <li>Starts Logstash with the generated configuration file.</li>
                        </ol>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">#!/bin/bash

# Export variables from .env file to environment variables
export $(cat /usr/share/logstash/pipeline/.env | xargs)

# Substitute variables in create-topic.sh and logstash.conf
envsubst &lt; /usr/share/logstash/pipeline/logstash.template.conf &gt; /usr/share/logstash/pipeline/logstash.conf

# Ensure the substituted files have the correct permissions
chown root:root /usr/share/logstash/pipeline/logstash.conf

# Execute the Python script
exec logstash -f /usr/share/logstash/pipeline/logstash.conf
</code></pre>
                        </div>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">9.6 logstash.template.conf [/project-directory/logstash]</h3>
                    <div class="content">
                        <p>The <strong><i>logstash.template.conf</i></strong> file is a Logstash configuration template that defines how data flows from Kafka (as the input) to Elasticsearch (as the output). This template uses placeholders (environment variables) to dynamically configure Kafka and Elasticsearch settings at runtime, making the setup flexible and adaptable to different environments without needing to modify the configuration file directly.</p>
                        <h6 style="color: #149ddd;"><strong>Input Section: Kafka</strong></h6>
                        <p>This section defines Kafka as the input source for Logstash, meaning Logstash will consume data from Kafka topics. The key settings here are:</p>
                        <ul>
                            <li><strong>bootstrap_servers:</strong> The Kafka broker address is specified dynamically using the <strong><i>${KAFKA_BOOTSTRAP_SERVERS}</i></strong> environment variable. In this case, it points to the Kafka instance defined in the <strong><i>.env</i></strong> file (e.g., <strong><i>kafka:9092</i></strong>), allowing Logstash to connect to the Kafka cluster for consuming messages.</li>
                            <li><strong>topics:</strong> Logstash will subscribe to the topic(s) specified in this configuration. Here, <strong><i>${KAFKA_TOPIC_1}</i></strong> is dynamically substituted with the actual Kafka topic name (e.g., <strong><i>bank_transactions</i></strong>), allowing Logstash to consume messages from the relevant Kafka topic. This topic is where the Python banking application sends the generated transaction data.</li>
                            <li><strong>codec:</strong> Specifies the format of the incoming Kafka messages. In this case, json is used, meaning the data consumed from Kafka is in JSON format. Logstash will parse these messages as JSON objects, making it easy to extract and transform fields for further processing.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">input {
  kafka {
    bootstrap_servers => "${KAFKA_BOOTSTRAP_SERVERS}"
    topics => ["${KAFKA_TOPIC_1}"]
    codec => "json"
  }
}
</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Output Section: Elasticsearch</strong></h6>
                        <p>This section defines Elasticsearch as the output destination for Logstash. After processing the data from Kafka, Logstash sends the data to Elasticsearch for indexing and storage. The key settings here are:</p>
                        <ul>
                            <li><strong>hosts:</strong> Specifies the Elasticsearch endpoint that Logstash will connect to. In this case, the host is the Elasticsearch coordination node <strong><i>(es-coord:9200)</i></strong>, which routes requests to the master nodes for indexing.</li>
                            <li><strong>user</strong> and <strong>password:</strong> These are dynamically populated from the environment variables <strong><i>${ES_USER}</i></strong> and <strong><i>${ES_PASSWORD}</i></strong>, respectively. They provide the credentials for Logstash to authenticate with Elasticsearch, using the default user <strong><i>(elastic)</i></strong> and password.</li>
                            <li><strong>ssl_enabled:</strong> Ensures that Logstash uses SSL/TLS encryption when communicating with Elasticsearch. This setting is crucial for securing communication between Logstash and Elasticsearch, especially when handling sensitive data.</li>
                            <li><strong>cacert:</strong> Points to the certificate authority (CA) certificate file <strong><i>(/usr/share/logstash/config/certs/ca/ca.crt)</i></strong>, which Logstash uses to verify the authenticity of the SSL connection to Elasticsearch. This certificate was generated during the setup process.</li>
                            <li><strong>index:</strong> Defines the Elasticsearch index where the processed data will be stored. The index name is dynamically set using the <strong><i>${ES_INDEX_1}</i></strong> environment variable, which could be something like <strong><i>bank_transactions</i></strong>. This makes it easy to route different data streams to different indices based on the environment or use case.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">output {
  elasticsearch {
    hosts => ["https://es-coord:9200"]
    user => "${ES_USER}"
    password => "${ES_PASSWORD}"
    ssl_enabled => true
    cacert => "/usr/share/logstash/config/certs/ca/ca.crt"
    index => "${ES_INDEX_1}"
  }
}
</code></pre>
                        </div>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">9.7 .env [/project-directory/banking]</h3>
                    <div class="content">
                        <p>This <strong><i>.env</i></strong> file under the banking directory defines the environment variables used by the Python banking application. The app simulates the generation of synthetic transaction data and sends it to Kafka for further processing by Logstash and Elasticsearch. These variables allow the banking application to connect to Kafka and specify the Kafka topics where the generated data will be published. Below is a detailed explanation of each environment variable:</p>
                        <h6 style="color: #149ddd;"><strong>KAFKA_BOOTSTRAP_SERVERS:</strong></h6>
                        <ul>
                            <li>Specifies the address of the Kafka broker that the banking application will connect to. In this case, <strong><i>kafka:9092</i></strong> refers to the Kafka broker running within the Docker network (the service name kafka), and <strong><i>9092</i></strong> is the default Kafka port for communication.</li>
                            <li>This allows the banking application to send transaction data to Kafka for ingestion.</li>
                        </ul>
                        <h6 style="color: #149ddd;"><strong>KAFKA_PORT:</strong></h6>
                        <ul>
                            <li>Defines the port number <strong><i>(9092)</i></strong> for Kafka communication. This is the standard port Kafka uses to listen for incoming connections from producers (like the banking app) and consumers (like Logstash). This ensures that the app knows which port to use to send its data to Kafka.</li>
                        </ul>
                        <h6 style="color: #149ddd;"><strong>KAFKA_TOPIC_1:</strong></h6>
                        <ul>
                            <li>Specifies the first Kafka topic <strong><i>(bank_transactions)</i></strong> to which the banking application will publish its generated banking transaction data.</li>
                            <li>Kafka topics are logical channels where producers (like the banking app) send data, and consumers (like Logstash) retrieve it. The use of <strong><i>${KAFKA_TOPIC_1}</i></strong> ensures that all banking-related transaction data is sent to the <strong><i>bank_transactions</i></strong> topic. </li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_PORT=9092
KAFKA_TOPIC_1=bank_transactions
</code></pre>
                        </div>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">9.8 Dockerfile [/project-directory/banking]</h3>
                    <div class="content">
                        <p>This <strong><i>Dockerfile</i></strong> defines the steps to build a Docker image for the Python banking application. The image is based on Python 3.9, and additional packages are installed to run the application, which generates and sends synthetic transaction data to Kafka. This <strong><i>Dockerfile</i></strong> includes steps to copy necessary files, install required Python libraries, and dynamically configure the Python script at runtime. Below is a detailed explanation of each instruction:</p>
                        <h6 style="color: #149ddd;"><strong>Base Image</strong></h6>
                        <p>This pulls the official Python 3.9 image from Docker Hub. This image includes Python and the necessary tools to run Python applications, making it a good starting point for building the banking app. The Python banking application will run on this version of Python.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">FROM python:3.9</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Working Directory</strong></h6>
                        <p>Sets the working directory inside the container to <strong><i>/usr/src/app</i></strong>. All subsequent commands, like copying files and running scripts, will be executed within this directory. This helps organize the application files within the container.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">WORKDIR /usr/src/app</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Copy the Requirements File</strong></h6>
                        <p>Copies the <strong><i>requirements.txt</i></strong> file from the local directory (where the <strong><i>Dockerfile</i></strong> resides) into the container’s working directory <strong><i>(/usr/src/app)</i></strong>.	The <strong><i>requirements.txt</i></strong> file lists all the Python libraries the application needs.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">COPY requirements.txt .</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Install Python Packages</strong></h6>
                        <ul>
                            <li>Installs the required Python packages listed in <strong><i>requirements.txt</i></strong> using <strong><i>pip</i></strong>. The <strong><i>--no-cache-dir</i></strong> option prevents caching of installed packages, which reduces the size of the final image.</li>
                            <li>These libraries might include packages for Kafka integration, JSON handling, and other dependencies required by the banking app.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">RUN pip install --no-cache-dir -r requirements.txt</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Install Additional System Packages</strong></h6>
                        <p>Installs additional system packages needed for the container. These utilities are useful for debugging, networking, and environment variable substitution:</p>
                        <ul>
                            <li><strong>gettext-base:</strong> Provides the envsubst tool for substituting environment variables in files.</li>
                            <li><strong>vim:</strong> A text editor for viewing/editing files inside the container.</li>
                            <li><strong>net-tools, inetutils-ping:</strong> Networking tools to check connectivity, resolve DNS, and troubleshoot issues inside the container.</li>
                            <li><strong>netcat-traditional:</strong> A utility for testing network connections (useful for debugging Kafka connections).</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">RUN apt-get update && apt-get install -y gettext-base vim net-tools inetutils-ping netcat-traditional</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Copy Files into the Container</strong></h6>
                        <p>These commands copy the necessary files from the local directory into the container:</p>
                        <ul>
                            <li><strong>.env:</strong> The environment variables file that contains Kafka configuration (e.g., Kafka broker address and topic names).</li>
                            <li><strong>banking_app.template.py:</strong> A template Python script for the banking app that generates synthetic transaction data and sends it to Kafka. This template will have placeholders for environment variables, such as Kafka topic names.</li>
                            <li><strong>entrypoint.sh:</strong> A shell script that substitutes environment variables into the Python script and runs it when the container starts.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">COPY .env /usr/src/app
COPY banking_app.template.py /usr/src/app
COPY entrypoint.sh /usr/src/app
</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Set File Permissions</strong></h6>
                        <p>Grants execution permissions to the <strong><i>entrypoint.sh</i></strong> script, allowing it to be executed when the container starts. This is necessary because the script will handle environment variable substitution and the execution of the Python application.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">RUN chmod +x /usr/src/app/entrypoint.sh</code></pre>
                        </div>
                        <h6 style="color: #149ddd;"><strong>Entry Point</strong></h6>
                        <p>Specifies the entry point for the container. When the container starts, it will execute the <strong><i>entrypoint.sh</i></strong> script.</p>
                        <p>This script will:</p>
                        <ul>
                            <li>Substitute environment variables into the Python template script <strong><i>(banking_app.template.py)</i></strong>.</li>
                            <li>Run the finalized Python script, which generates and sends synthetic transaction data to Kafka.</li>
                        </ul>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">ENTRYPOINT ["/usr/src/app/entrypoint.sh"]</code></pre>
                        </div>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml"># Use official Python image
FROM python:3.9

# Set working directory
WORKDIR /usr/src/app

# Copy the requirements file
COPY requirements.txt .

# Install required Python packages
RUN pip install --no-cache-dir -r requirements.txt

# Install requied packages
RUN apt-get update && apt-get install -y gettext-base vim net-tools inetutils-ping netcat-traditional

# Copy .env file and template Python script into the container
COPY .env /usr/src/app
COPY banking_app.template.py /usr/src/app
COPY entrypoint.sh /usr/src/app

# Give execution rights to the script
RUN chmod +x /usr/src/app/entrypoint.sh

# Substitute environment variables in the Python script
ENTRYPOINT ["/usr/src/app/entrypoint.sh"]
</code></pre>
                        </div>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">9.9 entrypoint.sh [/project-directory/banking]</h3>
                    <div class="content">
                        <p>The <strong><i>entrypoint.sh</i></strong> script is responsible for dynamically configuring the banking application at container startup. It reads environment variables from the <strong><i>.env</i></strong> file, substitutes these values into the Python application template, and then launches the application with the finalized configuration. This script is crucial for ensuring that the banking application operates correctly with dynamically set values.</p>
                        <p>The <strong><i>entrypoint.sh</i></strong> script performs the following key tasks:</p>
                        <ol>
                            <li>Exports environment variables from the <strong><i>.env</i></strong> file, making them available for use in the script.</li>
                            <ul>
                                <li>The script reads the <strong><i>.env</i></strong> file and exports its variables to the environment. This ensures that all necessary configuration values are available as environment variables.</li>
                            </ul>
                            <li>Substitutes the environment variables into the Python application template <strong><i>(banking_app.template.py)</i></strong>, generating the final <strong><i>banking_app.py</i></strong> file.</li>
                            <ul>
                                <li>Using the envsubst command, the script replaces placeholders in the template file with actual environment variable values, creating a fully configured <strong><i>banking_app.py</i></strong> file.</li>
                            </ul>
                            <li>Ensures correct permissions on the generated Python file so that it can be executed.</li>
                            <ul>
                                <li>The script sets execute permissions on the <strong><i>banking_app.py</i></strong> file to ensure it can be run as a script.</li>
                            </ul>
                            <li>Starts the Python application with the generated configuration file.</li>
                            <ul>
                                <li>Finally, the script executes the <strong><i>banking_app.py</i></strong> file using Python, launching the banking application with the dynamically configured settings.</li>
                            </ul>
                        </ol>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">#!/bin/bash

# Export variables from .env file to environment variables
export $(cat /usr/src/app/.env | xargs)

# Substitute variables in cbanking_app.py and logstash.conf
envsubst &lt; /usr/src/app/banking_app.template.py &gt; /usr/src/app/banking_app.py

# Ensure the substituted files have the correct permissions
chmod +x /usr/src/app/banking_app.py

# Ensure the substituted file has the correct permissions
chmod +x banking_app.py

# Execute the Python script
exec python3 banking_app.py
</code></pre>
                        </div>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">9.10 banking_app.template.py [/project-directory/banking]</h3>
                    <div class="content">
                        <p>The banking_app.template.py script is a template for generating a Python script that simulates banking transactions and publishes them to a Kafka topic. The environment variables in this template are replaced by the entrypoint.sh script at container startup, ensuring that the application is configured with the correct values. This script is crucial for generating realistic transaction data and testing the integration with Kafka.</p>
                        <p>The banking_app.template.py script performs the following key tasks:</p>
                        <ol>
                            <li>Imports necessary libraries and initializes the logger.</li>
                            <ul>
                                <li><strong>Purpose:</strong> Imports various libraries required for generating transaction data, handling dates and times, logging, and interacting with Kafka.</li>
                                <li><strong>Logger Initialization:</strong> Sets up a logger to log information and errors to a file named banking_app.log.</li>
                            </ul>
                            <div class="code-block">
                                <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                                <pre><code class="language-bash">import csv
import json
import time
import random
import os
import logging
from datetime import datetime, timedelta
from faker import Faker
from kafka import KafkaProducer
from random import choice, uniform, randint

# Initialize Logger
logging.basicConfig(filename="banking_app.log", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
</code></pre>
                            </div>
                            <li>Defines country, currency, and city data.</li>
                            <ul>
                                <li><strong>Purpose:</strong> Defines a dictionary COUNTRIES_DATA containing information about different countries, including their currency, cities, and country codes.</li>
                            </ul>
                            <div class="code-block">
                                <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                                <pre><code class="language-bash"># Initialize Faker instance
fake = Faker()

# Define Country, Currency, and City Data
COUNTRIES_DATA = {
    "United States": {
        "currency": "USD",
        "cities": ["New York City", "Los Angeles", "Chicago", "Houston", "San Francisco"],
        "country_code": "+1"
    },
    "Canada": {
        "currency": "CAD",
        "cities": ["Toronto", "Vancouver", "Montreal", "Calgary", "Ottawa"],
        "country_code": "+1"
    },
    "United Kingdom": {
        "currency": "GBP",
        "cities": ["London", "Manchester", "Birmingham", "Glasgow", "Edinburgh"],
        "country_code": "+44"
    },
    "Australia": {
        "currency": "AUD",
        "cities": ["Sydney", "Melbourne", "Brisbane", "Perth", "Adelaide"],
        "country_code": "+61"
    },
    "Germany": {
        "currency": "EUR",
        "cities": ["Berlin", "Munich", "Frankfurt", "Hamburg", "Cologne"],
        "country_code": "+49"
    },
    "France": {
        "currency": "EUR",
        "cities": ["Paris", "Marseille", "Lyon", "Toulouse", "Nice"],
        "country_code": "+33"
    },
    "Italy": {
        "currency": "EUR",
        "cities": ["Rome", "Milan", "Florence", "Venice", "Naples"],
        "country_code": "+39"
    },
    "Japan": {
        "currency": "JPY",
        "cities": ["Tokyo", "Osaka", "Kyoto", "Yokohama", "Nagoya"],
        "country_code": "+81"
    },
    "China": {
        "currency": "CNY",
        "cities": ["Beijing", "Shanghai", "Guangzhou", "Shenzhen", "Chengdu"],
        "country_code": "+86"
    },
    "India": {
        "currency": "INR",
        "cities": ["Delhi", "Mumbai", "Hyderabad", "Pune", "Dehradun"],
        "country_code": "+91"
    },
    "Brazil": {
        "currency": "BRL",
        "cities": ["São Paulo", "Rio de Janeiro", "Brasília", "Salvador", "Fortaleza"],
        "country_code": "+55"
    },
    "South Africa": {
        "currency": "ZAR",
        "cities": ["Johannesburg", "Cape Town", "Durban", "Pretoria", "Port Elizabeth"],
        "country_code": "+27"
    },
    "Mexico": {
        "currency": "MXN",
        "cities": ["Mexico City", "Guadalajara", "Monterrey", "Cancún", "Puebla"],
        "country_code": "+52"
    },
    "Russia": {
        "currency": "RUB",
        "cities": ["Moscow", "Saint Petersburg", "Novosibirsk", "Yekaterinburg", "Kazan"],
        "country_code": "+7"
    },
    "Spain": {
        "currency": "EUR",
        "cities": ["Madrid", "Barcelona", "Valencia", "Seville", "Bilbao"],
        "country_code": "+34"
    },
    "South Korea": {
        "currency": "KRW",
        "cities": ["Seoul", "Busan", "Incheon", "Daegu", "Gwangju"],
        "country_code": "+82"
    },
    "Turkey": {
        "currency": "TRY",
        "cities": ["Istanbul", "Ankara", "Izmir", "Bursa", "Antalya"],
        "country_code": "+90"
    },
    "Saudi Arabia": {
        "currency": "SAR",
        "cities": ["Riyadh", "Jeddah", "Mecca", "Medina", "Khobar"],
        "country_code": "+966"
    },
    "Argentina": {
        "currency": "ARS",
        "cities": ["Buenos Aires", "Córdoba", "Rosario", "Mendoza", "Mar del Plata"],
        "country_code": "+54"
    },
    "Egypt": {
        "currency": "EGP",
        "cities": ["Cairo", "Alexandria", "Giza", "Luxor", "Aswan"],
        "country_code": "+20"
    },
    "Thailand": {
        "currency": "THB",
        "cities": ["Bangkok", "Chiang Mai", "Phuket", "Pattaya", "Krabi"],
        "country_code": "+66"
    },
    "Indonesia": {
        "currency": "IDR",
        "cities": ["Jakarta", "Bali", "Surabaya", "Bandung", "Medan"],
        "country_code": "+62"
    },
    "Vietnam": {
        "currency": "VND",
        "cities": ["Hanoi", "Ho Chi Minh City", "Da Nang", "Haiphong", "Nha Trang"],
        "country_code": "+84"
    }
}
</code></pre>
                            </div>
                            <li>Generates a single banking transaction record.</li>
                            <ul>
                                <li><strong>Purpose:</strong> The generate_transaction function creates a synthetic transaction record with various details such as transaction ID, timestamp, response time, mode of transaction, currency, country, city, transaction amount, transaction type, account type, payment method, transaction status, customer info, and merchant info.</li>
                            </ul>
                            <div class="code-block">
                                <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                                <pre><code class="language-bash"># Function to generate a single banking transaction record
def generate_transaction():
    country = choice(list(COUNTRIES_DATA.keys()))
    city = choice(COUNTRIES_DATA[country]['cities'])
    currency = COUNTRIES_DATA[country]['currency']
    country_code = COUNTRIES_DATA[country]['country_code']

    transaction = {
        "transaction_id": fake.uuid4(),
        "transaction_timestamp": fake.iso8601(),
        "ingest_timestamp": datetime.now().strftime("%Y-%m-%dT%H:%M:%S"),
        "transaction_response_time": round(random.uniform(0.1, 2.5), 3),
        "mode_of_transaction": random.choice(["credit_card", "debit_card", "online_transfer", "ATM_withdrawal"]),
        "currency": currency,
        "country": country,
        "city": city,
        "transaction_amount": round(random.uniform(5.0, 5000.0), 2),
        "transaction_type": random.choice(["deposit", "withdrawal", "transfer"]),
        "account_type": random.choice(["checking", "savings", "credit_card"]),
        "payment_method": random.choice(["cash", "card", "online_banking"]),
        "transaction_status": random.choice(["successful", "failed", "pending"]),
        "customer_info": {
            "name": fake.name(),
            "age": random.randint(18, 75),
            "occupation": fake.job(),
            "mobile_number": f"{country_code}{fake.msisdn()[len(country_code):]}",
            "email": fake.email()
        },
        "merchant_info": {
            "name": fake.company(),
            "category": random.choice(["grocery", "electronics", "fashion", "entertainment", "food"]),
            "mobile_number": f"{country_code}{fake.msisdn()[len(country_code):]}",
            "email": fake.company_email()
        }
    }
    return transaction
</code></pre>
                            </div>
                            <li>Sets up Kafka producer.</li>
                            <ul>
                                <li><strong>Purpose:</strong> The create_kafka_producer function attempts to create a Kafka producer that connects to the Kafka broker specified by the environment variable ${KAFKA_BOOTSTRAP_SERVERS}.</li>
                                <li><strong>Retries:</strong> It retries the connection up to 5 times if the initial connection fails, logging warnings and errors as needed.</li>
                            </ul>
                            <div class="code-block">
                                <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                                <pre><code class="language-bash"># Kafka setup
def create_kafka_producer():
    retries = 5
    producer = None
    while retries > 0:
        try:
            producer = KafkaProducer(bootstrap_servers=['${KAFKA_BOOTSTRAP_SERVERS}'], value_serializer=lambda v: json.dumps(v).encode('utf-8'))
            logger.info("Kafka connection established")
            return producer
        except Exception as e:
            logger.warning(f"Kafka is not ready, retrying... ({5 - retries + 1}/5)")
            retries -= 1
            time.sleep(60)

    if producer is None:
        logger.error("Failed to connect to Kafka after multiple retries.")
        raise ConnectionError("Failed to connect to Kafka after multiple retries.")
    return producer
</code></pre>
                            </div>
                            <li>Publishes data to Kafka.</li>
                            <ul>
                                <li><strong>Purpose:</strong> The publish_to_kafka function sends the generated transaction data to a specified Kafka topic and flushes the producer to ensure the data is sent. It logs the success or failure of each publish attempt.</li>
                            </ul>
                            <div class="code-block">
                                <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                                <pre><code class="language-bash"># Function to publish data to Kafka
def publish_to_kafka(producer, topic, message):
    try:
        producer.send(topic, message)
        producer.flush()
        logger.info(f"Data published to Kafka topic {topic}")
    except Exception as e:
        logger.error(f"Failed to publish message to Kafka: {str(e)}")
</code></pre>
                            </div>
                            <li>Main execution block.</li>
                            <ul>
                                <li><strong>Purpose:</strong> The script creates a Kafka producer and enters an infinite loop where it generates and publishes batches of 100 transactions to Kafka.</li>
                                <li><strong>Publishing Transactions:</strong> For each transaction in the batch, it publishes the transaction to the Kafka topic specified by the environment variable ${KAFKA_TOPIC_1} and waits for 0.5 seconds before publishing the next transaction.</li>
                                <li><strong>Error Handling:</strong> The script handles interruptions and errors gracefully, logging an appropriate message if the script is interrupted by the user or if an error occurs.</li>
                            </ul>
                            <div class="code-block">
                                <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                                <pre><code class="language-bash">if __name__ == "__main__":
    producer = create_kafka_producer()

    while True:
        try:
            transactions = [generate_transaction() for _ in range(100)]
            for transaction in transactions:
                publish_to_kafka(producer, "${KAFKA_TOPIC_1}", transaction)
                time.sleep(0.5)

        except KeyboardInterrupt:
            logger.info("Script interrupted by user. Exiting...")
            break
        except Exception as e:
            logger.error(f"An error occurred: {str(e)}")
</code></pre>
                            </div>
                        </ol>
                    </div>
                    <h3 style="margin-top: 0.5em;" class="collapsible subheading">9.11 requirements.txt [/project-directory/banking]</h3>
                    <div class="content">
                        <p>The requirements.txt file is a vital part of the project, listing all the Python dependencies needed for the application. It ensures that anyone working on the project can easily install the necessary packages, fostering a consistent development environment.</p>
                        <p>In the <strong><i>Dockerfile</i></strong>, this file is used to install the required Python packages during the Docker image build process. This guarantees that the Docker container has all the dependencies needed to run the application.</p>
                        <p>By managing the project’s dependencies, the requirements.txt file simplifies the setup process for new developers and ensures consistent application performance across different environments. Its integration in the <strong><i>Dockerfile</i></strong> automates the installation of dependencies, making the Docker container ready to run the application with all necessary packages installed.</p>
                        <div class="code-block">
                            <button class="copy-button"><i class="fas fa-copy"></i> Copy</button>
                            <pre><code class="language-yaml">kafka-python
faker
</code></pre>
                        </div>
                    </div>
                </div>
            </section>
        </main>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-yaml.min.js"></script>
        <script>
            // 1. Collapsible sections functionality. single click expands, double click collapses
            document.querySelectorAll('.collapsible').forEach(coll => {
              const content = coll.nextElementSibling;

              coll.addEventListener('click', function () {
                // Expand on single click only if not already expanded
                if (content.style.display !== 'block') {
                  this.classList.add('active');
                  content.style.display = 'block';
                }
              });

              coll.addEventListener('dblclick', function () {
                // Collapse on double click
                this.classList.remove('active');
                content.style.display = 'none';
              });
            });

            // 2. Copy button functionality
            document.querySelectorAll('.copy-button').forEach(button => {
              button.addEventListener('click', () => {
                const codeBlock = button.nextElementSibling.querySelector('code');
                if (codeBlock) {
                  navigator.clipboard.writeText(codeBlock.textContent.trim());
                  button.textContent = 'Copied!';
                  setTimeout(() => button.textContent = 'Copy', 2000);
                }
              });
            });

            // 3. Enhanced smooth scroll + expand collapsible section
            document.querySelectorAll('a[href^="#"]').forEach(anchor => {
              anchor.addEventListener('click', function (e) {
                const target = document.querySelector(this.getAttribute('href'));
                if (!target) return;

                e.preventDefault();

                // Scroll into view
                target.scrollIntoView({
                  behavior: 'smooth',
                  block: 'start'
                });

                // Expand collapsible inside target section
                const collapsibleHeading = target.querySelector('.collapsible');
                if (collapsibleHeading) {
                  const content = collapsibleHeading.nextElementSibling;
                  const isCollapsed = content && content.style.display !== 'block';

                  if (isCollapsed) {
                    collapsibleHeading.classList.add('active');
                    content.style.display = 'block';
                  }
                }
              });
            });

            // 4. Optional: Submenu toggles (keep as is)
            document.addEventListener('DOMContentLoaded', function () {
              const toggleSubmenuLinks = document.querySelectorAll('.toggle-submenu');
              toggleSubmenuLinks.forEach(link => {
                link.addEventListener('click', function () {
                  const submenu = this.nextElementSibling;
                  if (submenu) {
                    submenu.style.display = submenu.style.display === 'block' ? 'none' : 'block';
                  }
                });
              });
            });

            // 5. Track last click timestamps per sidebar link
            const clickTimestamps = {};

            document.querySelectorAll('a[href^="#"]').forEach(anchor => {
              anchor.addEventListener('click', function (e) {
                const href = this.getAttribute('href');
                const target = document.querySelector(href);
                if (!target) return;

                e.preventDefault();

                const now = Date.now();
                const lastClick = clickTimestamps[href] || 0;
                const timeSinceLastClick = now - lastClick;
                clickTimestamps[href] = now;

                // Scroll to the target section
                target.scrollIntoView({
                  behavior: 'smooth',
                  block: 'start'
                });

                const collapsible = target.querySelector('.collapsible');
                const content = collapsible ? collapsible.nextElementSibling : null;

                if (collapsible && content) {
                  // Collapse all other collapsible sections
                  document.querySelectorAll('.collapsible').forEach(other => {
                    const otherContent = other.nextElementSibling;
                    if (other !== collapsible && otherContent) {
                      other.classList.remove('active');
                      otherContent.style.display = 'none';
                    }
                  });

                  // Double-click: collapse target
                  if (timeSinceLastClick < 300) {
                    collapsible.classList.remove('active');
                    content.style.display = 'none';
                  } else {
                    // Single-click: expand if not already
                    if (content.style.display !== 'block') {
                      collapsible.classList.add('active');
                      content.style.display = 'block';
                    }
                  }
                }
              });
            });

        </script>
    </body>
</html>